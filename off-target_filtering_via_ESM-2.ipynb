{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Based upon the following article: Predicting Protein-Protein\n",
    "Interactions Using a Protein Language Model and Linear Sum Assignment\n",
    "(https://huggingface.co/blog/AmelieSchreiber/protein-binding-partners\n",
    "-with-esm2)\n",
    "\"\"\"\n",
    "\n",
    "# Importing required libraries\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "from transformers import AutoTokenizer, EsmForMaskedLM\n",
    "import torch\n",
    "import pandas as pd\n",
    "from biotite.database import uniprot\n",
    "import biotite.sequence.io.fasta as fasta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EsmForMaskedLM(\n",
       "  (esm): EsmModel(\n",
       "    (embeddings): EsmEmbeddings(\n",
       "      (word_embeddings): Embedding(33, 320, padding_idx=1)\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "      (position_embeddings): Embedding(1026, 320, padding_idx=1)\n",
       "    )\n",
       "    (encoder): EsmEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-5): 6 x EsmLayer(\n",
       "          (attention): EsmAttention(\n",
       "            (self): EsmSelfAttention(\n",
       "              (query): Linear(in_features=320, out_features=320, bias=True)\n",
       "              (key): Linear(in_features=320, out_features=320, bias=True)\n",
       "              (value): Linear(in_features=320, out_features=320, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (rotary_embeddings): RotaryEmbedding()\n",
       "            )\n",
       "            (output): EsmSelfOutput(\n",
       "              (dense): Linear(in_features=320, out_features=320, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (LayerNorm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (intermediate): EsmIntermediate(\n",
       "            (dense): Linear(in_features=320, out_features=1280, bias=True)\n",
       "          )\n",
       "          (output): EsmOutput(\n",
       "            (dense): Linear(in_features=1280, out_features=320, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (LayerNorm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (emb_layer_norm_after): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (contact_head): EsmContactPredictionHead(\n",
       "      (regression): Linear(in_features=120, out_features=1, bias=True)\n",
       "      (activation): Sigmoid()\n",
       "    )\n",
       "  )\n",
       "  (lm_head): EsmLMHead(\n",
       "    (dense): Linear(in_features=320, out_features=320, bias=True)\n",
       "    (layer_norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "    (decoder): Linear(in_features=320, out_features=33, bias=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialising the model and tokeniser\n",
    "tokeniser = AutoTokenizer.from_pretrained(\"facebook/esm2_t6_8M_UR50D\")\n",
    "model = EsmForMaskedLM.from_pretrained(\"facebook/esm2_t6_8M_UR50D\")\n",
    "\n",
    "# Ensure that the model is in evaluation mode\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if a GPU is available and if so, set the model to run on it\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HVIDB has 456 recorded human-VACV PPIs; for each of them, the Masked\n",
    "# Language Model (MLM) loss is computed in order to determine a suitable\n",
    "# threshold value\n",
    "# As a first step, import the respective CSV file and extract the\n",
    "# interaction pairs, which are stored in the column \"Human-virus PPI\"\n",
    "HVIDB_VACV_interactions_df = pd.read_csv(\n",
    "    \"all_HVIDB_VACV_interactions.csv\"\n",
    ")\n",
    "interaction_pair_IDs = HVIDB_VACV_interactions_df[\n",
    "    \"Human-virus PPI\"\n",
    "].to_list()\n",
    "\n",
    "# Querying the UniProt database is not possible for the following IDs:\n",
    "# A0A0A0MR88, H0Y4G9 and A0A087X117 (they are all human protein IDs)\n",
    "# Therefore, the interactions they are involved in are removed from the\n",
    "# list\n",
    "elements_to_remove = []\n",
    "\n",
    "for interaction in interaction_pair_IDs:\n",
    "    if (\n",
    "        (\"A0A0A0MR88\" in interaction)\n",
    "        or\n",
    "        (\"H0Y4G9\" in interaction)\n",
    "        or\n",
    "        (\"A0A087X117\" in interaction)\n",
    "    ):\n",
    "        elements_to_remove.append(interaction)\n",
    "\n",
    "for element in elements_to_remove:\n",
    "    interaction_pair_IDs.remove(element)\n",
    "\n",
    "# In the individual strings representing an interaction pair, the human\n",
    "# and VACV UniProt ID are separated from each other by a hyphen\n",
    "split_IDs_list = [\n",
    "    pair_str.split(\"-\") for pair_str in interaction_pair_IDs\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The largest combined sequence length is 36196.\n"
     ]
    }
   ],
   "source": [
    "# Determine the largest combined sequence length among the HVIDB\n",
    "# interaction pairs\n",
    "combined_lengths_list = []\n",
    "\n",
    "for interaction_pair in split_IDs_list:\n",
    "    human_ID, virus_ID = interaction_pair\n",
    "\n",
    "    # Take the colon into account\n",
    "    combined_length = 1\n",
    "\n",
    "    # Suspend code execution for 2 seconds in order to obviate\n",
    "    # server-side errors\n",
    "    #time.sleep(2)\n",
    "    io_object_human = uniprot.fetch(human_ID, \"fasta\")\n",
    "    # Read the StringIO object into a FASTA file\n",
    "    human_fasta = fasta.FastaFile.read(io_object_human)\n",
    "    # The FASTA file contains only one entry; hence, the first and only\n",
    "    # element is retrieved from the iterator returned by the `items()`\n",
    "    # method\n",
    "    _, seq_str = list(human_fasta.items())[0]\n",
    "    combined_length += len(seq_str)\n",
    "\n",
    "    #time.sleep(2)\n",
    "    io_object_virus = uniprot.fetch(virus_ID, \"fasta\")\n",
    "    virus_fasta = fasta.FastaFile.read(io_object_virus)\n",
    "    _, seq_str = list(virus_fasta.items())[0]\n",
    "    combined_length += len(seq_str)\n",
    "\n",
    "    combined_lengths_list.append(combined_length)\n",
    "\n",
    "print(\n",
    "    f\"The largest combined sequence length is {max(combined_lengths_list)}.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[616, 1149, 388, 280, 528, 474, 809, 1047, 585, 1122, 930, 685, 689, 568, 673, 798, 873, 635, 654, 718, 459, 1825, 812, 438, 1204, 553, 544, 280, 396, 635, 532, 922, 906, 309, 705, 472, 546, 657, 1217, 953, 482, 852, 615, 1055, 420, 617, 478, 639, 1278, 977, 758, 7090, 594, 964, 543, 1513, 856, 725, 237, 824, 944, 713, 447, 286, 2514, 1012, 1421, 497, 472, 919, 1861, 1449, 1305, 387, 2602, 640, 34555, 347, 420, 1383, 449, 2530, 1357, 1016, 347, 1060, 1156, 1111, 474, 715, 281, 525, 1598, 405, 703, 644, 353, 321, 678, 557, 915, 462, 382, 474, 2496, 913, 577, 407, 1217, 812, 599, 1046, 652, 636, 1412, 1836, 874, 795, 1185, 507, 2551, 1781, 1752, 2048, 1085, 1063, 298, 795, 398, 553, 1950, 1055, 3005, 1603, 280, 472, 604, 263, 278, 420, 267, 489, 847, 1398, 572, 472, 245, 2447, 405, 563, 1916, 2692, 1348, 355, 327, 460, 554, 344, 412, 912, 828, 632, 713, 2002, 347, 631, 472, 843, 808, 789, 776, 647, 1683, 653, 1013, 382, 582, 808, 465, 654, 313, 1122, 569, 824, 936, 280, 498, 1259, 476, 488, 922, 400, 2253, 439, 395, 653, 2540, 646, 636, 348, 482, 401, 741, 370, 337, 253, 1132, 264, 814, 347, 433, 2799, 357, 840, 705, 537, 267, 615, 342, 472, 1160, 398, 738, 818, 476, 1261, 406, 476, 590, 482, 1019, 1266, 2514, 474, 495, 688, 418, 1135, 310, 2200, 1440, 2590, 616, 461, 591, 429, 5971, 3429, 929, 225, 740, 906, 263, 274, 1715, 577, 313, 623, 476, 472, 463, 1125, 1132, 544, 625, 292, 911, 36196, 625, 313, 472, 279, 2704, 765, 410, 316, 245, 261, 2814, 1080, 2687, 588, 337, 406, 688, 601, 617, 331, 382, 278, 1224, 1449, 1227, 443, 286, 340, 2105, 1102, 640, 253, 489, 451, 314, 927, 1057, 463, 900, 640, 3576, 280, 476, 555, 292, 768, 2496, 554, 1073, 523, 717, 1113, 382, 539, 323, 828, 681, 278, 445, 472, 472, 505, 3041, 278, 482, 767, 375, 402, 1417, 922, 636, 902, 449, 280, 720, 825, 1214, 758, 3963, 385, 1390, 449, 581, 316, 1517, 395, 999, 419, 280, 984, 1211, 713, 625, 1774, 329, 1989, 2532, 635, 635, 644, 340, 3559, 1581, 476, 476, 999, 672, 475, 581, 263, 709, 478, 776, 874, 437, 354, 1407, 1275, 402, 648, 2683, 718, 482, 425, 640, 3079, 640, 1761, 533, 940, 1121, 742, 489, 428, 714, 313, 804, 316, 895, 985, 466, 790, 399, 891, 365, 1142, 963, 774, 582, 263, 761, 3062, 3963, 667, 1367, 537, 642, 632, 2592, 279, 430, 475, 515, 578, 477, 816, 1178, 2067, 314, 286, 758, 1461, 573, 578, 1923, 775, 664, 551, 1849, 756, 395, 1333, 456, 434, 421]\n"
     ]
    }
   ],
   "source": [
    "print(combined_lengths_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "print(np.count_nonzero(np.array(combined_lengths_list) > 4000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_mlm_loss(protein_1, protein_2, max_length, iterations=3):\n",
    "    \"\"\"\n",
    "    Computes the Masked Language Model (MLM) loss between a pair of\n",
    "    proteins using the ESM-2 model in order to assess the probability of\n",
    "    interaction between the respective proteins.\n",
    "\n",
    "    Usage of this function assumes ESM-2 to have already been\n",
    "    initialised and to be stored in a variable named \"model\".\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    protein_1: str\n",
    "        A string representing the amino acid sequence of the first\n",
    "        protein in one-letter code.\n",
    "    protein_2: str\n",
    "        A string representing the amino acid sequence of the second\n",
    "        protein in one-letter code.\n",
    "    max_length: int\n",
    "        The maximum combined sequence length including the colon\n",
    "        character.\n",
    "    iterations: int, optional\n",
    "        The amount of times the procedure of randomly masking tokens and\n",
    "        computing the MLM loss is repeated.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    avg_mlm_loss: float\n",
    "        The average MLM loss for the given pair of proteins.\n",
    "    \"\"\"\n",
    "    total_loss = 0.0\n",
    "\n",
    "    for _ in range(iterations):\n",
    "        # Concatenate the two protein sequences with a colon as\n",
    "        # separator between them\n",
    "        concatenated_sequence = protein_1 + \":\" + protein_2\n",
    "\n",
    "        # Mask a subset of amino acids in the concatenated sequence\n",
    "        # (excluding the separator)\n",
    "        tokens = list(concatenated_sequence)\n",
    "        mask_rate = 0.15 # Masking 15% of the sequence; optimal values\n",
    "        # should be determined empirically/heuristically by testing\n",
    "        # different values\n",
    "        num_masks = int(len(tokens) * mask_rate)\n",
    "\n",
    "        # Exclude the separator from potential mask indices\n",
    "        available_indices = [\n",
    "            i for i, token in enumerate(tokens) if token != \":\"\n",
    "        ]\n",
    "        probs = torch.ones(len(available_indices))\n",
    "        mask_indices = torch.multinomial(probs, num_masks, replacement=False)\n",
    "\n",
    "        # Note that an intermediate step is taken by first indexing the\n",
    "        # list `available_indices` instead of indexing the list `tokens`\n",
    "        # directly\n",
    "        # This is due to the fact that there is no one-to-one\n",
    "        # correspondence between the indices randomly chosen by\n",
    "        # `torch.multinomial` and the indices of the tokens as the colon\n",
    "        # is not taken into account\n",
    "        for idx in mask_indices:\n",
    "            tokens[available_indices[idx]] = tokeniser.mask_token\n",
    "        \n",
    "        masked_sequence = \"\".join(tokens)\n",
    "        inputs = tokeniser(\n",
    "            masked_sequence,\n",
    "            return_tensors=\"pt\",\n",
    "            truncation=True,\n",
    "            max_length=max_length,\n",
    "            padding=\"max_length\"\n",
    "        )\n",
    "\n",
    "        # Comppute the MLM loss\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs, labels=inputs[\"input_ids\"])\n",
    "            loss = outputs.loss\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    # Return the average loss\n",
    "    return total_loss / iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the MLM loss for the first ten interaction pairs\n",
    "first_ten_interaction_pairs = split_IDs_list[:10]\n",
    "mlm_loss_list = []\n",
    "\n",
    "for interaction_pair in first_ten_interaction_pairs:\n",
    "    # Retrieve for each UniProt ID the corresponding sequence by\n",
    "    # querying the UniProt database\n",
    "    seqs = []\n",
    "    for uniprot_ID in interaction_pair:\n",
    "        io_object = uniprot.fetch(uniprot_ID, \"fasta\")\n",
    "        fasta_file = fasta.FastaFile.read(io_object)\n",
    "        _, seq_str = list(fasta_file.items())[0]\n",
    "        seqs.append(seq_str)\n",
    "    \n",
    "    human_seq, virus_seq = seqs\n",
    "    \n",
    "    avg_loss = compute_mlm_loss(human_seq, virus_seq, max_length=4000)\n",
    "\n",
    "    mlm_loss_list.append(avg_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16.05976454416911, 12.061362584431967, 18.526925404866535, 19.77448018391927, 17.920093536376953, 18.357943852742512, 15.585258801778158, 13.492405891418457, 17.451945622762043, 12.384026209513346]\n"
     ]
    }
   ],
   "source": [
    "print(mlm_loss_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Values for max_length=1000\n",
      "17.667570114135742\n",
      "10.302842132250467\n",
      "4.833453504918497\n"
     ]
    }
   ],
   "source": [
    "print(\"Values for max_length=1000\")\n",
    "print(max(mlm_loss_list))\n",
    "print(np.mean(mlm_loss_list))\n",
    "print(np.std(mlm_loss_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Values for max_length=2000\n",
      "19.77448018391927\n",
      "16.161420663197834\n",
      "2.5841445831938548\n"
     ]
    }
   ],
   "source": [
    "print(\"Values for max_length=2000\")\n",
    "print(max(mlm_loss_list))\n",
    "print(np.mean(mlm_loss_list))\n",
    "print(np.std(mlm_loss_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Values for max_length=3000\n",
      "20.974369049072266\n",
      "18.607375717163087\n",
      "1.5803331755458345\n"
     ]
    }
   ],
   "source": [
    "print(\"Values for max_length=3000\")\n",
    "print(max(mlm_loss_list))\n",
    "print(np.mean(mlm_loss_list))\n",
    "print(np.std(mlm_loss_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Values for max_length=4000\n",
      "21.16938845316569\n",
      "19.61531899770101\n",
      "1.049258936351101\n"
     ]
    }
   ],
   "source": [
    "print(\"Values for max_length=4000\")\n",
    "print(max(mlm_loss_list))\n",
    "print(np.mean(mlm_loss_list))\n",
    "print(np.std(mlm_loss_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bioinformatics",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
