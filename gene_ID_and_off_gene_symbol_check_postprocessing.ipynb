{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The Python script \"gene_ID_and_off_gene_symbol_check.py\" has been\n",
    "successfully run on the Hemera HPC cluster. However, on throwing a\n",
    "glance at the output file and subsequently scutinising the\n",
    "\"ID_manufacturer\" column in the CSV file, several issues emerged.\n",
    "\n",
    "The first is that the database query failed for four gene IDs, namely\n",
    "644862, 441848, 441931 and 441860.\n",
    "\n",
    "The second is that for some perturbation agents, be they siRNA, pooled\n",
    "siRNA or esiRNA, more than one target gene is listed. In both the column\n",
    "\"ID_manufacturer\" and \"Name\", the individual entries are separated by\n",
    "semicolons.\n",
    "\n",
    "The third is that for some perturbation agents, the entry in both\n",
    "\"ID_manufacturer\" and \"Name\" is \"Not available\". However, as both the\n",
    "total amount of perturbation agents this applies to is manageable and\n",
    "the catalogue number of along with the manufacturer is provided, the\n",
    "target genes are manually looked up.\n",
    "\n",
    "As to the first two issues, however, postprocessing is accomplished in\n",
    "an automated manner.\n",
    "\n",
    "Apart from that, some records have been discontinued in the NCBI\n",
    "database, such as the record corresponding to the gene ID 441848. Such\n",
    "discontinued records contain the sentence \"This record was\n",
    "discontinued\". It is decided at a later time how discontinued records\n",
    "are dealt with.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import string\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from biotite.database import entrez"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the screen data\n",
    "# Bear in mind that for certain columns, the data type has to be\n",
    "# manually specified\n",
    "dtype_dict = {\n",
    "    \"Ensembl_ID_OnTarget_Ensembl_GRCh38_release_87\": str,\n",
    "    \"Ensembl_ID_OnTarget_NCBI_HeLa_phs000643_v3_p1_c1_HMB\": str,\n",
    "    \"Gene_Description\": str,\n",
    "    \"ID\": str,\n",
    "    \"ID_OnTarget_Ensembl_GRCh38_release_87\": str,\n",
    "    \"ID_OnTarget_Merge\": str,\n",
    "    \"ID_OnTarget_NCBI_HeLa_phs000643_v3_p1_c1_HMB\": str,\n",
    "    \"ID_OnTarget_RefSeq_20170215\": str,\n",
    "    \"ID_manufacturer\": str,\n",
    "    \"Name_alternatives\": str,\n",
    "    \"PLATE_QUALITY_DESCRIPTION\": str,\n",
    "    \"RefSeq_ID_OnTarget_RefSeq_20170215\": str,\n",
    "    \"Seed_sequence_common\": str,\n",
    "    \"WELL_QUALITY_DESCRIPTION\": str,\n",
    "    \"siRNA_error\": str,\n",
    "    \"siRNA_number\": str,\n",
    "    \"Precursor_Name\": str\n",
    "}\n",
    "\n",
    "# Dask DataFrames exhibit a peculiarity regarding the index labels: By\n",
    "# default, the index labels are integers, just as with Pandas\n",
    "# DataFrames. However, unlike Pandas DataFrames, the index labels do not\n",
    "# monotonically increase from 0, but restart at 0 for each partition,\n",
    "# thereby resulting in duplicated index labels (Dask subdivides a Dask\n",
    "# DataFram into multiple so-called partitions as the whole idea behind\n",
    "# Dask is to handle large data sets in a memory-efficient way, https://\n",
    "# docs.dask.org/en/stable/generated/dask.dataframe.DataFrame.reset_\n",
    "# index.html)\n",
    "# Hence, performing operations with Dask DataFrames might potentially\n",
    "# raise the `ValueError: cannot reindex on an axis with duplicate\n",
    "# labels` error\n",
    "# In this case, loading the entire data set into a Pandas DataFrame is\n",
    "# feasible, which is why this is preferred to loading it into a Dask\n",
    "# DataFrame (strangely enough, this has not been possible in the very\n",
    "# beginning, which is why Dask was used in the first place)\n",
    "main_csv_df = pd.read_csv(\n",
    "    \"VacciniaReport_20170223-0958_ZScored_conc_and_NaN_adjusted.csv\",\n",
    "    sep=\"\\t\",\n",
    "    dtype=dtype_dict\n",
    ")\n",
    "\n",
    "# Bear in mind that due to operator precedence, i.e. \"|\" (logical OR)\n",
    "# having precedence over equality checks, the equality checks have to be\n",
    "# surrounded by parentheses\n",
    "single_pooled_siRNA_and_esiRNA_df = main_csv_df.loc[\n",
    "    (main_csv_df[\"WellType\"] == \"SIRNA\")\n",
    "    |\n",
    "    (main_csv_df[\"WellType\"] == \"POOLED_SIRNA\")\n",
    "    |\n",
    "    (main_csv_df[\"WellType\"] == \"ESIRNA\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Amount of perturbation agents for which the target genes are not specified: 22\n"
     ]
    }
   ],
   "source": [
    "# Determine the amount of perturbation agents for which the target genes\n",
    "# are not specified\n",
    "ID_manufacturer_series = single_pooled_siRNA_and_esiRNA_df[\n",
    "    \"ID_manufacturer\"\n",
    "]\n",
    "\n",
    "n_target_not_specified = np.count_nonzero(\n",
    "    ID_manufacturer_series == \"Not available\"\n",
    ")\n",
    "\n",
    "print(\n",
    "    \"Amount of perturbation agents for which the target genes are not \"\n",
    "    f\"specified: {n_target_not_specified}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following features/columns contain commata in their entries:\n",
      "Feature name      Numeric index Alphabetical Index\n",
      "--------------------------------------------------\n",
      "Name_alternatives 30            AE\n",
      "Gene_Description  62            BK"
     ]
    }
   ],
   "source": [
    "# Now, address the first two issues in an automated manner (failed\n",
    "# database query for four gene IDs and the listing of several target\n",
    "# genes separated by semicolons)\n",
    "# To this end, the CSV file with the updated gene IDs and official gene\n",
    "# symbols is also loaded\n",
    "# Unfortunately, when saving the updated Pandas DataFrame to a CSV file,\n",
    "# the separator has not been specified, which is why the default\n",
    "# separator has been used\n",
    "# This, however, conflicts with the usage of commata in some entries\n",
    "# Thus, prior to loading the updated CSV file, the commata have to be\n",
    "# replaced with tab stops in a sophisticated manner taking account of\n",
    "# this difference between actual delimiters and commata which are part\n",
    "# of entries\n",
    "# To this end, columns containing commata in their entries have to be\n",
    "# identified\n",
    "feature_names = main_csv_df.columns\n",
    "features_with_commata = []\n",
    "features_with_commata_indices = []\n",
    "\n",
    "for i, feature_name in enumerate(feature_names):\n",
    "    feature_series = main_csv_df[feature_name]\n",
    "    if feature_series.dtype != \"object\":\n",
    "        continue\n",
    "    # Bear in mind that in order to check for the presence of a\n",
    "    # substring in a Pandas DataFrame, \"pandas.Series.str.contains\" has\n",
    "    # to be used rather than \"pandas.Series.isin\" as the latter only\n",
    "    # verifies complete matches between column entries and query strings\n",
    "    n_commata_in_entries = feature_series.str.contains(\",\").sum()\n",
    "    if n_commata_in_entries > 0:\n",
    "        features_with_commata.append(feature_name)\n",
    "        features_with_commata_indices.append(i)\n",
    "\n",
    "# Excel uses upper case letters instead of numbers in order to index\n",
    "# columns\n",
    "# Hence, for the sake of convenience, the numeric indices are\n",
    "# simultaneously mapped to the corresponding alphabetical indices\n",
    "# The built-in string module allows to fetch a string representing the\n",
    "# entire alphabet\n",
    "alphabet_list = list(string.ascii_uppercase)\n",
    "\n",
    "alphabetic_indices_list = list(string.ascii_uppercase)\n",
    "for first_letter in alphabet_list[:3]:\n",
    "    for second_letter in alphabet_list:\n",
    "        alphabetic_indices_list.append(first_letter + second_letter)\n",
    "\n",
    "numeric_alphabetic_index_dict = {}\n",
    "for numeric_index, alphabetic_index in enumerate(alphabetic_indices_list):\n",
    "    numeric_alphabetic_index_dict[numeric_index] = alphabetic_index\n",
    "\n",
    "max_feature_name_length = max(map(len, features_with_commata))\n",
    "\n",
    "print(\n",
    "    \"The following features/columns contain commata in their entries:\\n\",\n",
    "    \"Feature name\".ljust(max_feature_name_length + 1),\n",
    "    \"Numeric index\".ljust(14),\n",
    "    \"Alphabetical Index\\n\",\n",
    "    \"-\" * (max_feature_name_length + 1 + 14 + len(\"Alphabetical index\")),\n",
    "    sep=\"\",\n",
    "    end=\"\"\n",
    ")\n",
    "\n",
    "for i, feature_name in zip(\n",
    "    features_with_commata_indices, features_with_commata\n",
    "):\n",
    "    print(\n",
    "        \"\\n\",\n",
    "        feature_name.ljust(max_feature_name_length + 1),\n",
    "        str(i).ljust(14),\n",
    "        numeric_alphabetic_index_dict[i],\n",
    "        sep=\"\",\n",
    "        end=\"\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['MultipleTargets' 'NoTargets' 'Not available' 'OK' 'POOLED_SIRNA_ERROR'\n",
      " 'TargetMismatch' 'Unknown']\n",
      "['ENST00000000233;ENST00000463733'\n",
      " 'ENST00000000233;ENST00000463733;ENST00000415666;ENST00000467281;ENST00000489673;ENST00000459680'\n",
      " 'ENST00000000233;ENST00000463733;ENST00000415666;ENST00000489673;ENST00000464403'\n",
      " ... 'ENST00000515849;ENST00000302763;ENST00000355078' 'ENST00000516084'\n",
      " 'Not available']\n"
     ]
    }
   ],
   "source": [
    "# Now, the entries of the columns immediately following those the\n",
    "# entries of which contain commata are scrutinised\n",
    "# Ideally, they exhibit common characteristics that can be leveraged for\n",
    "# the distinction between actual delimiters and commata belonging to\n",
    "# entries\n",
    "following_series_1 = main_csv_df[feature_names[31]]\n",
    "following_series_2 = main_csv_df[feature_names[63]]\n",
    "\n",
    "unique_vals_series_1 = np.unique(following_series_1)\n",
    "unique_vals_series_2 = np.unique(following_series_2)\n",
    "\n",
    "print(unique_vals_series_1)\n",
    "print(unique_vals_series_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "# Regarding the second of the two investigated columns, it emerges that\n",
    "# the vast majority of its entries begin with the sequence \"ENST\"\n",
    "# It is investigated whether this is indeed the case for all entries or\n",
    "# whether there are some exceptions\n",
    "print(all([entry[:4] == \"ENST\" for entry in unique_vals_series_2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Not available']\n"
     ]
    }
   ],
   "source": [
    "# Apparently, there are entries not starting with the \"ENST\" sequence\n",
    "# They are extracted and examined\n",
    "# The arguably easiest way to accomplish this is boolean indexing, which\n",
    "# is provided by NumPy\n",
    "outcast_vals_arr = unique_vals_series_2[\n",
    "    [entry[:4] != \"ENST\" for entry in unique_vals_series_2]\n",
    "]\n",
    "print(outcast_vals_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It becomes apparent that I am damn lucky as I indeed am able to\n",
    "# leverage common characterics in order to distinguish actual delimiters\n",
    "# from commata belonging to entries\n",
    "\n",
    "# List comprising unique values of column 31, i.e. the column following\n",
    "# column 30 (\"Name_alternatives\")\n",
    "siRNA_error_options = [\n",
    "    \"MultipleTargets\",\n",
    "    \"NoTargets\",\n",
    "    \"Not available\",\n",
    "    \"OK\",\n",
    "    \"POOLED_SIRNA_ERROR\",\n",
    "    \"TargetMismatch\",\n",
    "    \"Unknown\"\n",
    "]\n",
    "\n",
    "# Iterate through the lines, modify them accordingly and write the\n",
    "# adjusted lines to a new output file\n",
    "# Note that reading all the lines into memory at once via `.readlines()`\n",
    "# provokes an Out Of Memory error, which is why the file lines are\n",
    "# iterated over on the fly\n",
    "with open(\n",
    "    \"Vaccinia_Report_NCBI_Gene_IDs_and_official_gene_symbols_updated.csv\",\n",
    "    \"r\"\n",
    ") as prior_tab_intro_file, open(\n",
    "    \"adjusted_file.csv\", \"w\", newline=\"\"\n",
    ") as post_tab_intro_file:\n",
    "    for i, line in enumerate(prior_tab_intro_file):\n",
    "        # Bear in mind that the first line represents the header, i.e.\n",
    "        # contains the column names\n",
    "        # Thus, all commata represent actual delimiters\n",
    "        if i == 0:\n",
    "            split_line = [\n",
    "                i for j in line.split(\",\") for i in (j, \",\")\n",
    "            ][:-1]\n",
    "\n",
    "            # Simply replace all commata with tab stops\n",
    "            split_line_with_tabs = [\n",
    "                \"\\t\" if i == \",\" else i for i in split_line\n",
    "            ]\n",
    "            \n",
    "            # Concatenate the entries in the updated list and write the\n",
    "            # resulting string to the file\n",
    "            post_tab_intro_file.write(\"\".join(split_line_with_tabs))\n",
    "            continue\n",
    "\n",
    "        # When employing the built-in split method for strings, the\n",
    "        # separation character is not retained, but discarded\n",
    "        # Hence, by employing a trick involving a nested list\n",
    "        # comprehension, the separation character is added at its\n",
    "        # corresponding positions\n",
    "        # (https://www.geeksforgeeks.org/python-string-split-including-spaces/)\n",
    "        split_line = [i for j in line.split(\",\") for i in (j, \",\")][:-1]\n",
    "        \n",
    "        line_comma_indices = [\n",
    "            i for i, x in enumerate(split_line) if x == \",\"\n",
    "        ]\n",
    "\n",
    "        # Determine the indices of commata belonging to entries in lieu\n",
    "        # of being delimiters\n",
    "        entry_commata_list = []\n",
    "\n",
    "        # First, deal with column 30, i.e. \"Name alternatives\"\n",
    "        # Keep in mind that it is iterated through the list\n",
    "        # `split_line`, which encompasses both the entries as well as\n",
    "        # commata\n",
    "        # Also keep in mind that the numeric index starts with zero, not\n",
    "        # 1, so that when counting in the \"human\" way, column 30 has\n",
    "        # index 31\n",
    "        # Therefore, the index corresponding to column 30 is not 30, but\n",
    "        # 30 * 2 = 60 (counting starts with 0, hence the first column\n",
    "        # has index 0; to account for the remaining 30 entries, 30 * 2\n",
    "        # is added, yielding 60, the index of the entry corresponding to\n",
    "        # column 30)\n",
    "        # Also bear in mind that the column has at least one entry,\n",
    "        # which is why the index of the first element to query is\n",
    "        # increased by two, i.e. 62\n",
    "        entry_index_1 = 62\n",
    "        subsequent_entry = split_line[entry_index_1]\n",
    "        while subsequent_entry not in siRNA_error_options:\n",
    "            entry_commata_list.append(entry_index_1 - 1)\n",
    "            entry_index_1 += 2\n",
    "            subsequent_entry = split_line[entry_index_1]\n",
    "        \n",
    "        # Now, do the same thing with column 62, i.e. \"Gene_Description\"\n",
    "        # Again, the index of the entry in `split_line` corresponding to\n",
    "        # column 62 is not 62, but 62 * 2 = 124, and as the\n",
    "        # column contains at least one entry, the index of the first\n",
    "        # entry to query is increased by two (126)\n",
    "        # Note that the index of the first entry to investigate has to\n",
    "        # be adjusted according to the previous amount of \"entry\n",
    "        # commata\"\n",
    "        entry_index_2 = 126 + len(entry_commata_list) * 2\n",
    "        subsequent_entry = split_line[entry_index_2]\n",
    "        while (\n",
    "            (subsequent_entry != \"Not available\")\n",
    "            and\n",
    "            (subsequent_entry[:4] != \"ENST\")\n",
    "        ):\n",
    "            entry_commata_list.append(entry_index_2 - 1)\n",
    "            entry_index_2 += 2\n",
    "            subsequent_entry = split_line[entry_index_2]\n",
    "        \n",
    "        # Update the list harbouring the row entries along with the\n",
    "        # delimiters by replacing commata with tab stops at the\n",
    "        # corresponding positions\n",
    "        for comma_index in line_comma_indices:\n",
    "            if comma_index not in entry_commata_list:\n",
    "                split_line[comma_index] = \"\\t\"\n",
    "        \n",
    "        # Finally, the entries in the updated row list are concatenated\n",
    "        # and the resulting string is written to the file\n",
    "        # As the `.readlines()` method does not trim line endings, the\n",
    "        # newline character (\\n) does not have to be added\n",
    "        post_tab_intro_file.write(\"\".join(split_line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "updated_main_csv_df = pd.read_csv(\n",
    "    \"Vaccinia_Report_NCBI_Gene_IDs_and_official_gene_symbols_\"\\\n",
    "    \"updated_with_tab_stops.csv\",\n",
    "    sep=\"\\t\",\n",
    "    dtype=dtype_dict\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Amount of manufacturer IDs occurring in conjunction with more than one value in the \"Name\" column: 1229\n"
     ]
    }
   ],
   "source": [
    "# I have a hunch that I might have have made a mistake: I assumed that\n",
    "# the two columns \"ID_manufacturer\" and \"ID\" refer to the same thing\n",
    "# This, however, is not necessarily the case, as it is also conceivable\n",
    "# that siRNA directed against a specific target has been employed in\n",
    "# order to knock down another, maybe structurally related target\n",
    "# Therefore, it is investigated whether this indeed is the case or\n",
    "# whether the siRNAs have exclusively been used for the targets\n",
    "# specified by the manufacturer\n",
    "# To this end, the unique values of the column \"ID_manufacturer\" are\n",
    "# determined, and for each unique value, the amount of unique values\n",
    "# occurring in the column \"Name\" in conjunction with that unique\n",
    "# \"ID_manufacturer\" are determined\n",
    "# Bear in mind that the original, non-updated CSV file has to be used\n",
    "# for this purpose\n",
    "unique_manufacturer_IDs = np.unique(main_csv_df[\"ID_manufacturer\"])\n",
    "\n",
    "non_unique_list = []\n",
    "for manufacturer_ID in unique_manufacturer_IDs:\n",
    "    names_series = main_csv_df.loc[\n",
    "        main_csv_df[\"ID_manufacturer\"] == manufacturer_ID\n",
    "    ][\"Name\"]\n",
    "    n_unique_names = len(np.unique(names_series))\n",
    "    if n_unique_names > 1:\n",
    "        non_unique_list.append(manufacturer_ID)\n",
    "\n",
    "print(\n",
    "    \"Amount of manufacturer IDs occurring in conjunction with more \"\n",
    "    f\"than one value in the \\\"Name\\\" column: {len(non_unique_list)}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the list harbouring the manufacturer IDs co-occurring with with\n",
    "# multiple \"Name\" values to a file\n",
    "with open(\"manufacturer_IDs_co-occurring_with_multiple_names.txt\", \"w\") as f:\n",
    "    for i, manufacturer_ID in enumerate(non_unique_list):\n",
    "        if i == 0:\n",
    "            f.write(manufacturer_ID)\n",
    "        else:\n",
    "            f.write(\"\\n\" + manufacturer_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Closer scrutiny of the affected manufacturer IDs reveals that the\n",
    "# occurrence of multiple \"Name\" values stems from the usage of aliases\n",
    "# for one and the same gene\n",
    "# Thus, this issue does not have to be further addressed, as it had been\n",
    "# resolved by the database query anyway"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unfortunately, another issue arose: I erroneously assumed that the two\n",
    "# columns \"ID_manufacturer\" and \"ID\" refer to the same thing\n",
    "# This, however, is not the case, which I found out in the following\n",
    "# way: For some IDs appearing in \"ID_manufacturer\", the \"Name\" column\n",
    "# can adopt more than one value; for instance, this is the case for the\n",
    "# ID 441848, which appears in conjunction with the the values\n",
    "# \"LOC441842\" and \"LOC441848\" in the \"Name\" column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['LOC441842']\n",
      "\n",
      "['LOC441842' 'LOC441848']\n",
      "\n",
      "['LOC441842' 'VN1R17P']\n",
      "\n",
      "['LOC441842' 'LOC441860']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Eventually, address the four gene IDs for which the database query\n",
    "# failed, which are 644862, 441848, 441931 and 441860\n",
    "failed_gene_IDs = [\"644862\", \"441848\", \"441931\", \"441860\"]\n",
    "\n",
    "for failed_gene_ID in failed_gene_IDs:\n",
    "    # Determine the gene name provided by the data set\n",
    "    name_in_data_set = np.unique(updated_main_csv_df.loc[\n",
    "        updated_main_csv_df[\"ID_manufacturer\"] == failed_gene_ID\n",
    "    ][\"Name\"])\n",
    "    # assert len(name_in_data_set) == 1, (\n",
    "    #     \"Strangely enough, more than one name occurs for gene ID \"\n",
    "    #     f\"{failed_gene_ID}.\"\n",
    "    # )\n",
    "    print(name_in_data_set)\n",
    "    print()\n",
    "    continue\n",
    "\n",
    "    time.sleep(2)\n",
    "\n",
    "    NCBI_entry = entrez.fetch_single_file(\n",
    "        uids=[failed_gene_ID],\n",
    "        file_name=None,\n",
    "        db_name=\"gene\",\n",
    "        ret_type=\"\",\n",
    "        ret_mode=\"text\"\n",
    "    )\n",
    "    NCBI_entry_str = NCBI_entry.getvalue()\n",
    "\n",
    "    # The gene ID has not been changed for any of the four gene IDs, so\n",
    "    # that is only has be checked whether the official gene symbol has\n",
    "    # been altered or not\n",
    "    # Remove blank lines from the string retrieved from the NCBI entry\n",
    "    NCBI_entry_str_list = NCBI_entry_str.split(\"\\n\")\n",
    "    while \"\" in NCBI_entry_str_list:\n",
    "        NCBI_entry_str_list.remove(\"\")\n",
    "\n",
    "    # Following the removal of empty strings, the official gene symbol\n",
    "    # is represented by the first list element, but it is preceded by\n",
    "    # the string \"1. \", which encompasses three characters\n",
    "    # Hence, prior to comparing the gene names provided by the VACV\n",
    "    # screen data set to the official gene symbols, the first list\n",
    "    # element has to be sliced accordingly\n",
    "    official_gene_symbol = NCBI_entry_str_list[0][:3]\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1. RPS28P3\n",
      "Official Symbol: RPS28P3 and Name: ribosomal protein S28 pseudogene 3 [Homo sapiens (human)]\n",
      "Other Aliases: RPS28_2_172\n",
      "Chromosome: 1; Location: 1q42.13\n",
      "Annotation: Chromosome 1 NC_000001.11 (228134782..228136793, complement)\n",
      "ID: 644862\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "1. LOC441848\n",
      "similar to zinc finger protein 113 [Homo sapiens (human)]\n",
      "Chromosome: 19; Location: 19q13.11\n",
      "This record was discontinued.\n",
      "ID: 441848\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "1. VN1R17P\n",
      "Official Symbol: VN1R17P and Name: vomeronasal 1 receptor 17 pseudogene [Homo sapiens (human)]\n",
      "Other Aliases: GPCR\n",
      "Other Designations: putative G-protein coupled receptor\n",
      "Chromosome: 1; Location: 1q44\n",
      "Annotation: Chromosome 1 NC_000001.11 (247237224..247237850, complement)\n",
      "ID: 441931\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "1. LOC441860\n",
      "similar to dJ54B20.4 (novel KRAB box containing C2H2 type zinc finger protein) [Homo sapiens (human)]\n",
      "Chromosome: 19; Location: 19q13.41\n",
      "This record was discontinued.\n",
      "ID: 441860\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "failed_gene_IDs = [\"644862\", \"441848\", \"441931\", \"441860\"]\n",
    "\n",
    "for failed_gene_ID in failed_gene_IDs:\n",
    "    time.sleep(2)\n",
    "\n",
    "    NCBI_entry = entrez.fetch_single_file(\n",
    "        uids=[failed_gene_ID],\n",
    "        file_name=None,\n",
    "        db_name=\"gene\",\n",
    "        ret_type=\"\",\n",
    "        ret_mode=\"text\"\n",
    "    )\n",
    "    NCBI_entry_str = NCBI_entry.getvalue()\n",
    "    print(NCBI_entry_str)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bioinformatics",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
