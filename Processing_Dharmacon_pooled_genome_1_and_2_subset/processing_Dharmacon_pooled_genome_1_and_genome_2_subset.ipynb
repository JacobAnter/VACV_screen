{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3d70317a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nThe purpose of this Jupyter notebook is to process the Dharmacon pooled\\nGenome 1 and Genome 2 subset of the VACV screen.\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "The purpose of this Jupyter notebook is to process the Dharmacon pooled\n",
    "Genome 1 and Genome 2 subset of the VACV screen.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8d984fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from biotite.sequence.io import fasta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0d40810",
   "metadata": {},
   "source": [
    "#### Extraction of Dharmacon Pooled Genome 1 & 2 Screening Plates Subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f5825579",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/j1/64kbg_f11z97kx1dw__420vh0000gn/T/ipykernel_3966/4203157425.py:6: DtypeWarning: Columns (37) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  entire_vacv_screen_df = pd.read_csv(\n"
     ]
    }
   ],
   "source": [
    "path_to_entire_vacv_screen = (\n",
    "    \"/Users/jacobanter/Documents/Code/VACV_screen/VacciniaReport_\"\n",
    "    \"20170223-0958_ZScored_conc_and_NaN_adjusted.csv\"\n",
    ")\n",
    "\n",
    "entire_vacv_screen_df = pd.read_csv(\n",
    "    path_to_entire_vacv_screen,\n",
    "    sep=\"\\t\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "46c7a051",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, extract the subset of interest, i.e. the Dharmacon pooled genome\n",
    "# 1 and 2 subset with screening plates and no checkerboard plates\n",
    "# Note that the `query()` method can be used instead of square brackets\n",
    "# in order to avoid entering the DataFrame name multiple times\n",
    "dp_g1_g2_subset_df = entire_vacv_screen_df.query(\n",
    "    \"(Experiment == 'VACCINIA-DP-G1' or Experiment == 'VACCINIA-DP-G2') \"\n",
    "    \"and \"\n",
    "    \"PLATE_TYPE == 'ScreeningPlate'\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "04068bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(dp_g1_g2_subset_df) == 43776, (\n",
    "    \"Something went wrong while extracting the subset from the screen!\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3c8fa77d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the Dharmacon pooled subset to a TSV file\n",
    "dp_g1_g2_subset_df.to_csv(\n",
    "    \"Dharmacon_pooled_G1_G2_screening_plates_subset.tsv\",\n",
    "    sep=\"\\t\",\n",
    "    index=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54630c3d",
   "metadata": {},
   "source": [
    "#### Determining Unique Gene IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3a08022f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Dharmacon pooled subset into a DataFrame\n",
    "path_to_dharmacon_pooled_subset = (\n",
    "    \"Dharmacon_pooled_G1_G2_screening_plates_subset.tsv\"\n",
    ")\n",
    "\n",
    "dp_g1_g2_subset_df = pd.read_csv(\n",
    "    path_to_dharmacon_pooled_subset,\n",
    "    sep=\"\\t\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3b3c95c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine the unique values in the \"ID_manufacturer\" column to query\n",
    "# NCBI Entrez with\n",
    "unique_ids = dp_g1_g2_subset_df[\"ID_manufacturer\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4a756960",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 18,042 unique gene ids.\n"
     ]
    }
   ],
   "source": [
    "print(f\"There are {len(unique_ids):,} unique gene ids.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3fb2db38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not available\n"
     ]
    }
   ],
   "source": [
    "for unique_id in unique_ids:\n",
    "    try:\n",
    "        int(unique_id)\n",
    "    except ValueError:\n",
    "        print(unique_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d852282f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In order to download information from the NCBI Entrez gene database,\n",
    "# these 18,042 gene ids are put into a text file with one gene ID per\n",
    "# line\n",
    "# Bear in mind that in the context of working with files, the `with`\n",
    "# context manager is preferred as it automatically takes care of closing\n",
    "# files, even in case of errors/exceptions\n",
    "with open(\"gene_ids.txt\", \"w\") as f:\n",
    "    for i, unique_id in enumerate(unique_ids):\n",
    "        # Note that the unique IDs may comprise some special values,\n",
    "        # such as \"Not available\"\n",
    "        # These need to be omitted by filtering for integers/integer\n",
    "        # sequences\n",
    "        try:\n",
    "            int(unique_id)\n",
    "        except ValueError:\n",
    "            continue\n",
    "\n",
    "        if i != (len(unique_ids) - 1):\n",
    "            f.write(unique_id + \"\\n\")\n",
    "        else:\n",
    "            f.write(unique_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bff87fce",
   "metadata": {},
   "source": [
    "#### Processing NCBI Entrez Gene Database Files from the FTP File Server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d556f855",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/j1/64kbg_f11z97kx1dw__420vh0000gn/T/ipykernel_1194/2071511180.py:11: DtypeWarning: Columns (6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  gene_info_df = pd.read_csv(\n"
     ]
    }
   ],
   "source": [
    "# As it turns out, downloading files directly from the NCBI Entrez FTP\n",
    "# file server is far more convenient than using the `datasets` CLI\n",
    "# Thus, two files have been downloaded, namely `gene_info.gz` as well as\n",
    "# `gene_history.gz`\n",
    "# The latter is required in order to identify withdrawn or replaced gene\n",
    "# IDs\n",
    "# However, both files contain entries for all species, which is why they\n",
    "# have to be filtered to include only human entries (taxonomic ID 9606)\n",
    "\n",
    "# First, process `gene_info`\n",
    "gene_info_df = pd.read_csv(\n",
    "    \"gene_info\",\n",
    "    sep=\"\\t\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e2913323",
   "metadata": {},
   "outputs": [],
   "source": [
    "gene_info_df = gene_info_df[\n",
    "    gene_info_df[\"#tax_id\"] == 9606\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "81c01c87",
   "metadata": {},
   "outputs": [],
   "source": [
    "gene_info_df.to_csv(\n",
    "    \"gene_info_human_9606.tsv\",\n",
    "    sep=\"\\t\",\n",
    "    index=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2f977e10",
   "metadata": {},
   "outputs": [],
   "source": [
    "gene_history_df = pd.read_csv(\n",
    "    \"gene_history\",\n",
    "    sep=\"\\t\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9aea2d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "gene_history_df = gene_history_df[\n",
    "    gene_history_df[\"#tax_id\"] == 9606\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "297954ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "gene_history_df.to_csv(\n",
    "    \"gene_history_human_9606.tsv\",\n",
    "    sep=\"\\t\",\n",
    "    index=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "52bc1d99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Additionally, two other files have been downloaded; they are required\n",
    "# for the mapping of genes to UniProt accessions, i.e. proteins\n",
    "# These two files are `gene2accession.gz` and\n",
    "# `gene_refseq_uniprotkb_collab.gz`\n",
    "# As for the two previous files, they have to be filtered to include\n",
    "# only human entries (taxonomic ID 9606)\n",
    "gene_refseq_uniprotkb_collab_df = pd.read_csv(\n",
    "    \"gene_refseq_uniprotkb_collab\",\n",
    "    sep=\"\\t\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fb862600",
   "metadata": {},
   "outputs": [],
   "source": [
    "gene_refseq_uniprotkb_collab_df = gene_refseq_uniprotkb_collab_df[\n",
    "    gene_refseq_uniprotkb_collab_df[\"NCBI_tax_id\"] == 9606\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "469ca303",
   "metadata": {},
   "outputs": [],
   "source": [
    "gene_refseq_uniprotkb_collab_df.to_csv(\n",
    "    \"gene_refseq_uniprotkb_collab_human_9606.tsv\",\n",
    "    sep=\"\\t\",\n",
    "    index=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3946018c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/j1/64kbg_f11z97kx1dw__420vh0000gn/T/ipykernel_1194/1663381539.py:1: DtypeWarning: Columns (6,8,9,10) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  gene2accession_df = pd.read_csv(\n"
     ]
    }
   ],
   "source": [
    "gene2accession_df = pd.read_csv(\n",
    "    \"gene2accession\",\n",
    "    sep=\"\\t\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d8d2af74",
   "metadata": {},
   "outputs": [],
   "source": [
    "gene2accession_df = gene2accession_df[\n",
    "    gene2accession_df[\"#tax_id\"] == 9606\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bb84d982",
   "metadata": {},
   "outputs": [],
   "source": [
    "gene2accession_df.to_csv(\n",
    "    \"gene2accession_human_9606.tsv\",\n",
    "    sep=\"\\t\",\n",
    "    index=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "961ed08d",
   "metadata": {},
   "source": [
    "#### Inserting Columns into the DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "67c0c9ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "dp_g1_g2_subset_df = pd.read_csv(\n",
    "    \"Dharmacon_pooled_G1_G2_screening_plates_subset.tsv\",\n",
    "    sep=\"\\t\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "518706de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following columns are supposed to be inserted into the DataFrame:\n",
    "# `Gene_type`, which, as its name already suggests, indicates the type\n",
    "# of the respecticve gene\n",
    "# `UniProt_IDs`, which stores the UniProt ID(s) associated with the\n",
    "# respective gene\n",
    "# `Withdrawn_by_NCBI`, which, as its name already implied, indicates\n",
    "# whether the gene ID is still valid or not\n",
    "# They are supposed to be inserted immediately after the\n",
    "# \"Name_alternatives\" column\n",
    "# Thus, its index has to be determined\n",
    "columns_list = dp_g1_g2_subset_df.columns.to_list()\n",
    "\n",
    "insertion_index = columns_list.index(\"Name_alternatives\") + 1\n",
    "\n",
    "dp_g1_g2_subset_df.insert(\n",
    "    loc=insertion_index,\n",
    "    column=\"Gene_type\",\n",
    "    value=\"Value not set\"\n",
    ")\n",
    "\n",
    "insertion_index += 1\n",
    "\n",
    "dp_g1_g2_subset_df.insert(\n",
    "    loc=insertion_index,\n",
    "    column=\"UniProt_IDs\",\n",
    "    value=\"Value not set\"\n",
    ")\n",
    "\n",
    "insertion_index += 1\n",
    "\n",
    "dp_g1_g2_subset_df.insert(\n",
    "    loc=insertion_index,\n",
    "    column=\"Withdrawn_by_NCBI\",\n",
    "    value=\"Value not set\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc54cfec",
   "metadata": {},
   "source": [
    "#### Conducting the TSV File Update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b98d9a33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'NCBI_Entrez_utils' from '/Users/jacobanter/Documents/Code/VACV_screen/Processing_Dharmacon_pooled_genome_1_and_2_subset/NCBI_Entrez_utils.py'>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from importlib import reload\n",
    "\n",
    "import NCBI_Entrez_utils as utils\n",
    "\n",
    "reload(utils)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "82657cf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_updater = utils.NCBI_Entrez_data_lookup(\n",
    "    \"gene_info_human_9606.tsv\",\n",
    "    \"gene_history_human_9606.tsv\",\n",
    "    \"gene2accession_human_9606.tsv\",\n",
    "    \"gene_refseq_uniprotkb_collab_human_9606.tsv\",\n",
    "    \"sec_ac.txt\",\n",
    "    \"uniprotkb_organism_id_9606_2025_09_19_all_human_prots_Swiss-Prot_\"\n",
    "    \"and_TrEMBL_uniprot_acc_headers.fasta\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "79c25fe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "updated_dp_g1_g2_subset_df = data_updater.check_gene_id_and_symbol(\n",
    "    dp_g1_g2_subset_df\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fa27d024",
   "metadata": {},
   "outputs": [],
   "source": [
    "updated_dp_g1_g2_subset_df = data_updater.add_uniprot_ids(\n",
    "    updated_dp_g1_g2_subset_df\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d9acf3d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "updated_dp_g1_g2_subset_df.to_csv(\n",
    "    \"Dharmacon_pooled_G1_G2_screening_plates_subset_updated.tsv\",\n",
    "    sep=\"\\t\",\n",
    "    index=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3d127af",
   "metadata": {},
   "source": [
    "#### Construction of PPI Pairs for Screen Refinement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "377a1ff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the updated TSV file into a Pandas DataFrame\n",
    "dp_g1_g2_subset_df = pd.read_csv(\n",
    "    \"Dharmacon_pooled_G1_G2_screening_plates_subset_updated.tsv\",\n",
    "    sep=\"\\t\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f0214c59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the UniProt accessions from the Dharmacon pooled subset\n",
    "# Bear in mind that many entries are composite entries with semicolons\n",
    "# as separator\n",
    "# Also remember that many UniProt accessions probably occur multiple\n",
    "# times, requiring the removal of redundancies via e.g. the Pandas\n",
    "# `.drop_duplicates()` method\n",
    "# Yet another layer of complexity stems from the fact that some entries\n",
    "# are `Nan`, necessitating filtering\n",
    "dp_accs_list = (\n",
    "    dp_g1_g2_subset_df[\"UniProt_IDs\"]\n",
    "    .dropna()\n",
    "    .str.split(\";\")\n",
    "    .explode()\n",
    "    .drop_duplicates()\n",
    "    .to_list()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d89138d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 43,639 UniProtKB protein accessions in the Dharmacon pooled G1/G2\n",
      "screening plates subset of the screen.\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    f\"There are {len(dp_accs_list):,} UniProtKB protein accessions in \"\n",
    "    \"the Dharmacon pooled G1/G2\\nscreening plates subset of the screen.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "06f4cd47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the FASTA file containing all 440 VACV WR proteins\n",
    "path_to_VACV_WR_fasta = (\n",
    "    \"uniprotkb_organism_id_10254_2025_06_19_all_VACV_WR_prots_uniprot_\"\n",
    "    \"only_header.fasta\"\n",
    ")\n",
    "\n",
    "VACV_WR_fasta = fasta.FastaFile.read(path_to_VACV_WR_fasta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ee3087d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the VACV WR UniProt accessions from the FASTA file\n",
    "VACV_WR_uniprot_accs = list(VACV_WR_fasta.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "85390000",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now that both the VACV WR UniProt accessions and the Dharmacon pooled\n",
    "# subset UniProt accessions have been extracted, they are combined such\n",
    "# that each and every human protein from the screen is paired with each\n",
    "# of the 440 VACV WR proteins\n",
    "# This can be achieved in a very elegant way using a Pandas cross join\n",
    "# (also called a Cartesian product)\n",
    "\n",
    "# Convert the lists into DataFrames\n",
    "human_df = pd.DataFrame({\"human_protein\": dp_accs_list})\n",
    "vacv_wr_df = pd.DataFrame({\"VACV_WR_protein\": VACV_WR_uniprot_accs})\n",
    "\n",
    "# Perform a cross join\n",
    "human_vacv_ppi_pairs_df = human_df.merge(\n",
    "    vacv_wr_df,\n",
    "    how=\"cross\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e207d6a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "human_vacv_ppi_pairs_df.to_csv(\n",
    "    \"Dharmacon_pooled_G1_G2_screening_plates_subset_human-VACV_WR_\"\n",
    "    \"PPI_pairs.tsv\",\n",
    "    sep=\"\\t\",\n",
    "    index=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c7b5789",
   "metadata": {},
   "source": [
    "#### Splitting the PPI Pairs TSV File into Chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9830b37f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the TSV file harbouring the PPI pairs into a Pandas DataFrame\n",
    "human_vacv_ppi_pairs_df = pd.read_csv(\n",
    "    \"Dharmacon_pooled_G1_G2_screening_plates_subset_human-VACV_WR_\"\n",
    "    \"PPI_pairs.tsv\",\n",
    "    sep=\"\\t\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aa5e8166",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of human-VACV WR PPI pairs: 19,201,160\n"
     ]
    }
   ],
   "source": [
    "n_ppi_pairs = len(human_vacv_ppi_pairs_df)\n",
    "\n",
    "print(\n",
    "    f\"Number of human-VACV WR PPI pairs: {n_ppi_pairs:,}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "506b815d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In total, there are roughly 19,200,000 PPI pairs\n",
    "# The maximum time for HPC jobs using GPUs is 48 hours, which may be too\n",
    "# short\n",
    "# Thus, the PPI pairs are split into chunks each encompassing 5 million\n",
    "# PPI pairs (6 hours of the 48 hours are reserved for loading the FASTA\n",
    "# file and the embeddings; the batch size is 64; one batch takes roughly\n",
    "# 1 second; 42 hours can be used for actual inference;\n",
    "# 42 hours * 60 * 60 = 151,200 seconds = 151,200 batches;\n",
    "# 151,200 batches * 64 = 9,676,800 PPI pairs â‰ˆ 9,600,000 PPI pairs;\n",
    "# thus, up to 9,000,000 PPI pairs per chunk would be possible)\n",
    "ppi_pair_chunks_dir_path = \"PPI_pair_chunks\"\n",
    "\n",
    "if not os.path.exists(ppi_pair_chunks_dir_path):\n",
    "    os.makedirs(ppi_pair_chunks_dir_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3995d99f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In total, there are 4 chunks.\n"
     ]
    }
   ],
   "source": [
    "# Determine the total number of chunks\n",
    "CHUNK_SIZE = 5_000_000\n",
    "\n",
    "n_chunks = math.ceil(n_ppi_pairs / CHUNK_SIZE)\n",
    "\n",
    "print(f\"In total, there are {n_chunks} chunks.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87070ac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, for each of the 4 chunks, extract the respective subset and save\n",
    "# it to a separate TSV file\n",
    "for i in range(n_chunks):\n",
    "    current_subset = human_vacv_ppi_pairs_df.iloc[\n",
    "        i * CHUNK_SIZE: (i + 1) * CHUNK_SIZE\n",
    "    ]\n",
    "\n",
    "    # Bear in mind that xCAPT5 expects TSV files not to have a header\n",
    "    current_subset.to_csv(\n",
    "        os.path.join(\n",
    "            ppi_pair_chunks_dir_path,\n",
    "            \"Dharmacon_pooled_G1_G2_screening_plates_subset_human-\"\n",
    "            f\"VACV_WR_PPI_pairs_chunk_{i}.tsv\"\n",
    "        ),\n",
    "        sep=\"\\t\",\n",
    "        index=False,\n",
    "        header=False\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71a9f1d9",
   "metadata": {},
   "source": [
    "#### FASTA File Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f92c3c97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a FASTA file comprising the 440 VACV WR protein sequences as\n",
    "# well as the sequences of the human proteins in the Dharmacon pooled\n",
    "# subset\n",
    "human_Dharmacon_and_VACV_WR_prots_fasta = fasta.FastaFile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f3c4f642",
   "metadata": {},
   "outputs": [],
   "source": [
    "for header, seq in VACV_WR_fasta.items():\n",
    "    human_Dharmacon_and_VACV_WR_prots_fasta[header] = seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "87a4e0b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In order to obtain the protein sequences of the human proteins, all\n",
    "# UniProt Swiss-Prot and UniProt TrEMBL sequences for Homo sapiens (tax\n",
    "# ID 9606) are downloaded from UniProt\n",
    "# Only canonical protein sequences are downloaded, i.e. no isoforms\n",
    "# The download was conducted on September 19th 2025\n",
    "\n",
    "# Load the FASTA file\n",
    "all_human_prots_fasta = fasta.FastaFile.read(\n",
    "    \"uniprotkb_organism_id_9606_2025_09_19_all_human_prots_Swiss-Prot_\"\n",
    "    \"and_TrEMBL.fasta\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "efe29380",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The FASTA file downloaded from UniProt still has default headers\n",
    "# The headers have to be modified to contain only the UniProt accession\n",
    "all_human_prots_simple_header_fasta = fasta.FastaFile()\n",
    "\n",
    "for header, seq in all_human_prots_fasta.items():\n",
    "    # Conveniently enough, the header elements are separated by pipes/\n",
    "    # vertical bars with the UniProt accession being the second element\n",
    "    uniprot_acc = header.split(\"|\")[1]\n",
    "    all_human_prots_simple_header_fasta[uniprot_acc] = seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "61ed2773",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the new FASTA file to disk\n",
    "all_human_prots_simple_header_fasta.write(\n",
    "    \"uniprotkb_organism_id_9606_2025_09_19_all_human_prots_Swiss-Prot_\"\n",
    "    \"and_TrEMBL_uniprot_acc_headers.fasta\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3dfc2578",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the FASTA file with simplified headers\n",
    "all_human_prots_fasta = fasta.FastaFile.read(\n",
    "    \"uniprotkb_organism_id_9606_2025_09_19_all_human_prots_Swiss-Prot_\"\n",
    "    \"and_TrEMBL_uniprot_acc_headers.fasta\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4972fc11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quickly verify that all human proteins present in the Dharmacon pooled\n",
    "# subset are also covered by the FASTA file\n",
    "# Python set lookups are faster\n",
    "uniprot_accs_in_fasta = set(all_human_prots_fasta.keys())\n",
    "\n",
    "coverage_list = [\n",
    "    acc in uniprot_accs_in_fasta\n",
    "    for acc in dp_accs_list\n",
    "]\n",
    "\n",
    "assert all(coverage_list), (\n",
    "    \"Not all human proteins in the Dharmacon pooled subset are covered \"\n",
    "    \"by the FASTA file!\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5a05b799",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finally, add the human protein sequences to the FASTA file\n",
    "for human_prot_acc in dp_accs_list:\n",
    "    human_Dharmacon_and_VACV_WR_prots_fasta[human_prot_acc] = (\n",
    "        all_human_prots_fasta[human_prot_acc]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c7115a28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform a sanity check\n",
    "assert (\n",
    "    len(human_Dharmacon_and_VACV_WR_prots_fasta)\n",
    "    ==\n",
    "    (440 + 43_639)\n",
    "), \"Something went wrong while populating the FASTA file!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "13cab3a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now that the sanity check has been successfully passed, save the FASTA\n",
    "# file to disk\n",
    "human_Dharmacon_and_VACV_WR_prots_fasta.write(\n",
    "    \"VACV_WR_and_Dharmacon_pooled_G1_G2_screening_plates_human_prots.fasta\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0d69b52",
   "metadata": {},
   "source": [
    "#### Generating PPI Pairs File and FASTA File for Missing UniProt<br>Accessions from PPI Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c3b5ec2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In the Jupyter notebook\n",
    "# `evaluation_of_xCAPT5_performance_on_DP_G1_G2_subset.ipynb`, it has\n",
    "# been discovered that a certain number of UniProt accessions present in\n",
    "# the combined PPI data set is not present in the screen subset TSV\n",
    "# file, which in turn implies that PPI predictions have not been made\n",
    "# for these UniProt accessions\n",
    "# Thus, the PPI prediction for these UniProt accessions is made in a\n",
    "# separate step\n",
    "# To this end, a PPI pairs TSV file as well as a FASTA file are\n",
    "# generated\n",
    "# Just as with the previous PPI pair generation, this PPI pairing is\n",
    "# performed in a combinatorial manner using a cross join\n",
    "vacv_wr_df = pd.DataFrame({\"VACV_WR_protein\": VACV_WR_uniprot_accs})\n",
    "\n",
    "# Create a list comprising the missing UniProt accessions from the\n",
    "# combined data set\n",
    "missing_uniprot_accs_list = [\n",
    "    # Missing UniProt accessions from confirmed positive PPI instances\n",
    "    \"V9GZ56\", \"Q99729\", \"A8MUS3\", \"E9PDI4\", \"Q93086\", \"Q8WWI1\",\n",
    "    \"A0A2R8Y5A3\", \"F8VVA7\", \"F8WBV6\", \"H3BSR6\",\n",
    "    # Missing UniProt accessions from reliable negative PPI instances\n",
    "    \"F5GYR3\", \"F8WE32\", \"U3KQ75\", \"G3V5S9\", \"G3V2M5\", \"F8VRX4\",\n",
    "    \"D6RJF7\", \"A0A8V8TMR1\", \"O60531\", \"E9PPY3\", \"J3QR28\", \"A0JLS5\",\n",
    "    \"B4DHA6\", \"B4DRX8\", \"F8WC81\", \"D6RC52\", \"B1AMU7\", \"R4GNH9\",\n",
    "    \"F8WDT8\", \"Q05CW7\", \"B3KVX2\", \"Q5VXM9\", \"U3KQN5\", \"F8W8T7\",\n",
    "    \"D6RC74\", \"Q96GC8\", \"A0A087WWQ2\", \"B4DQC7\", \"V9GYP5\", \"D6RC60\",\n",
    "    \"B4E098\", \"C9J6C5\", \"C9JJU7\", \"B4E263\", \"B3KWS1\", \"A2VDI1\",\n",
    "    \"B7ZAU8\", \"Q8N7L7\", \"Q96ES5\", \"D6RBR7\", \"D6R8Y9\", \"D6R9C8\",\n",
    "    \"F6VJE8\", \"J3QR85\", \"J3KSR7\", \"A0A8V8TPK8\", \"A0A8V8TQT0\",\n",
    "    \"A0A8V8TP28\", \"A0A8V8TPD4\", \"F8WFE7\", \"B4DXL4\", \"M0R0P1\", \"M0R2U2\",\n",
    "    \"M0R1H0\", \"M0R2B0\", \"M0QYK9\", \"A0A8I5KT77\", \"B4DMU5\", \"C9JZT7\",\n",
    "    \"A0PJ56\", \"H0YBV6\", \"B3KPN5\", \"B2RE66\", \"B4DM91\", \"F5GWN9\",\n",
    "    \"B4DP15\", \"A0A1U9X8U3\", \"A8K806\", \"A8K9A1\", \"B4DNI0\", \"H0Y6G3\",\n",
    "    \"A0A0G2JJL1\", \"A0A140T9L0\", \"H7BZ72\", \"B3KN82\", \"A0A1B0GTK2\",\n",
    "    \"P0DW28\", \"Q05DN1\", \"A8MYC1\", \"A0A0A0MQS4\", \"A5D904\", \"B4DPI9\",\n",
    "    \"H0Y9L8\", \"B4DW33\", \"E7EX70\", \"O60747\", \"B4DHR2\", \"A0PJ87\",\n",
    "    \"I3L3U9\", \"I3L234\", \"B4E303\", \"B4E074\", \"A0A0A0MRH0\", \"Q5VU10\",\n",
    "    \"B4DJR3\", \"H0Y9Y4\", \"E9PKP7\", \"B4DNQ1\", \"E9PLY7\", \"A0A7I2V506\",\n",
    "    \"A0A7I2V5M5\", \"A0A7I2V2U7\", \"A0A7I2V349\", \"A0A7I2V699\", \"B7Z284\",\n",
    "    \"B7Z9G4\", \"U3KQ48\", \"A0A8Q3WK70\", \"A0A8Q3WKH7\", \"A0A994J4Y2\",\n",
    "    \"Q53GY5\", \"V9GYY5\", \"Q4G0D9\", \"Q6DKJ9\", \"A0A075B729\", \"A0A8I5QKX4\",\n",
    "    \"H7C446\", \"A0A8Q3SHT6\"\n",
    "]\n",
    "\n",
    "missing_uniprot_accs_df = pd.DataFrame({\n",
    "    \"human_protein\": missing_uniprot_accs_list\n",
    "})\n",
    "\n",
    "# Finally, perform the cross join\n",
    "missing_human_accs_vacv_ppi_pairs_df = missing_uniprot_accs_df.merge(\n",
    "    vacv_wr_df,\n",
    "    how=\"cross\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "218655aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bear in mind that xCAPT5 expects TSV files not to have a header\n",
    "missing_human_accs_vacv_ppi_pairs_df.to_csv(\n",
    "    \"missing_human_UniProt_accs_from_combined_PPI_data_set-VACV_WR_\"\n",
    "    \"PPI_pairs.tsv\",\n",
    "    sep=\"\\t\",\n",
    "    index=False,\n",
    "    header=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "67eb7674",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, turn to the generation of the FASTA file\n",
    "missing_human_accs_vacv_wr_fasta = fasta.FastaFile()\n",
    "\n",
    "# Add the 440 VACV WR protein sequences\n",
    "for header, seq in VACV_WR_fasta.items():\n",
    "    missing_human_accs_vacv_wr_fasta[header] = seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ea848995",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the protein sequences of the missing human UniProt accessions\n",
    "# However, prior to that, quickly verify that all missing human proteins\n",
    "# are covered by the FASTA file comprising the entire human proteome\n",
    "# Python set lookups are faster\n",
    "uniprot_accs_in_human_proteome = set(all_human_prots_fasta.keys())\n",
    "\n",
    "missing_human_prots_coverage_list = [\n",
    "    acc in uniprot_accs_in_human_proteome\n",
    "    for acc in missing_uniprot_accs_list\n",
    "]\n",
    "\n",
    "assert all(missing_human_prots_coverage_list), (\n",
    "    \"Not all missing human proteins are covered by the FASTA file \"\n",
    "    \"comprising the entire human proteome!\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f21a1493",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now that the sanity check has successfully been passed, add the\n",
    "# sequences of the missing human proteins\n",
    "for human_prot_acc in missing_uniprot_accs_list:\n",
    "    missing_human_accs_vacv_wr_fasta[human_prot_acc] = (\n",
    "        all_human_prots_fasta[human_prot_acc]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "117b5150",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform a sanity check regarding the number of entries in the FASTA\n",
    "# file\n",
    "assert (\n",
    "    len(missing_human_accs_vacv_wr_fasta)\n",
    "    ==\n",
    "    (440 + len(missing_uniprot_accs_list))\n",
    "), \"Something went wrong while populating the FASTA file!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7cf95028",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the FASTA file to disk\n",
    "missing_human_accs_vacv_wr_fasta.write(\n",
    "    \"VACV_WR_and_missing_human_prots_from_combined_PPI_data_set.fasta\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e33345e",
   "metadata": {},
   "source": [
    "#### Incorporating Missing UniProt Accessions from the PPI Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3faa5b73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The missing human UniProt accessions still need to be added to the\n",
    "# `UniProt_IDs` column of the screen subset TSV file\n",
    "# For this purpose, the following approach is devised: A dictionary\n",
    "# mapping gene names to the missing human UniProt accessions is created\n",
    "# Using the Pandas DataFrame `.apply()` method, the missing human\n",
    "# UniProt accessions are added to the `UniProt_IDs` entries of the\n",
    "# corresponding genes; the method implemented for this purpose uses the\n",
    "# abovementioned dictionary\n",
    "missing_gene_name_to_human_acc_dict = {\n",
    "    # Missing UniProt accessions from confirmed positive PPI instances\n",
    "    \"LSM4\": [\"V9GZ56\"],\n",
    "    \"HNRNPAB\": [\"Q99729\"],\n",
    "    \"RPL23A\": [\"A8MUS3\"],\n",
    "    \"LAD1\": [\"E9PDI4\"],\n",
    "    \"P2RX5\": [\"Q93086\"],\n",
    "    \"LMO7\": [\"Q8WWI1\"],\n",
    "    \"CTNNB1\": [\"A0A2R8Y5A3\"],\n",
    "    \"COPZ1\": [\"F8VVA7\"],\n",
    "    \"SERF2\": [\"F8WBV6\"],\n",
    "    \"CX3CL1\": [\"H3BSR6\"],\n",
    "    # Missing UniProt accessions from reliable negative PPI instances\n",
    "    \"FBL\": ['M0R0P1', 'M0R2U2', 'M0R1H0', 'M0R2B0'],\n",
    "    \"UTP25\": ['B3KVX2'],\n",
    "    \"GTPBP4\": ['O60747', 'B4DHR2'],\n",
    "    \"NEK11\": ['D6RJF7'],\n",
    "    \"FRG1\": ['E9PLY7'],\n",
    "    \"PHF8\": ['A0A8I5KT77'],\n",
    "    \"RPS9\": ['A5D904'],\n",
    "    \"ZNF501\": ['B2RE66'],\n",
    "    \"POLR1E\": ['B4DW33', 'E7EX70'],\n",
    "    \"TAF1B\": ['F8WE32', 'U3KQ75'],\n",
    "    \"POP4\": ['A8MYC1', 'A0A0A0MQS4'],\n",
    "    \"RIOX2\": ['H0Y9L8'],\n",
    "    \"FCF1\": ['G3V5S9', 'G3V2M5'],\n",
    "    \"RPS19BP1\": ['F8WFE7'],\n",
    "    \"NOP14\": ['Q96GC8'],\n",
    "    \"RBM10\": ['P0DW28'],\n",
    "    \"DGCR8\": ['A0A994J4Y2', 'Q53GY5'],\n",
    "    \"BOP1\": ['Q4G0D9', 'Q6DKJ9', 'A0A075B729'],\n",
    "    \"MPHOSPH10\": ['U3KQ48', 'A0A8Q3WK70', 'A0A8Q3WKH7'],\n",
    "    \"PRKDC\": ['A0A8V8TMR1'],\n",
    "    \"NOP16\": ['D6RC60', 'B4E098'],\n",
    "    \"PPAN\": ['H7C446'],\n",
    "    \"NOP2\": ['F5GYR3'],\n",
    "    \"NOL6\": ['B3KPN5'],\n",
    "    \"NCL\": ['A0A7I2V506', 'A0A7I2V5M5', 'A0A7I2V2U7', 'A0A7I2V349', 'A0A7I2V699'],\n",
    "    \"RPF2\": ['Q5VXM9', 'U3KQN5'],\n",
    "    \"SLX9\": ['C9JJU7'],\n",
    "    \"NLE1\": ['B4E303', 'B4E074', 'A0A0A0MRH0'],\n",
    "    \"NHP2\": ['D6RC52'],\n",
    "    \"NOL11\": ['J3QR28'],\n",
    "    \"WDR46\": ['B4DP15', 'A0A1U9X8U3', 'A8K806', 'A8K9A1', 'B4DNI0', 'H0Y6G3', 'A0A0G2JJL1', 'A0A140T9L0'],\n",
    "    \"RRP8\": ['E9PPY3'],\n",
    "    \"WDR75\": ['F8WC81'],\n",
    "    \"EMG1\": ['A0A087WWQ2', 'B4DQC7', 'V9GYP5'],\n",
    "    \"DEDD2\": ['M0QYK9'],\n",
    "    \"RSL1D1\": ['A0PJ87', 'I3L3U9', 'I3L234'],\n",
    "    \"DDX56\": ['F8WDT8'],\n",
    "    \"ZBTB11\": ['A0A8I5QKX4'],\n",
    "    \"UTP14A\": ['O60531'],\n",
    "    \"GNL2\": ['B4DPI9'],\n",
    "    \"HEATR1\": ['B4E263', 'B3KWS1', 'A2VDI1', 'B7ZAU8', 'Q8N7L7', 'Q96ES5'],\n",
    "    \"NOP58\": ['H7BZ72', 'B3KN82'],\n",
    "    \"TBL3\": ['A0JLS5'],\n",
    "    \"NOL12\": ['V9GYY5'],\n",
    "    \"ZCCHC7\": ['Q05DN1'],\n",
    "    \"GNL3\": ['B4DMU5', 'C9JZT7'],\n",
    "    \"RPP30\": ['Q5VU10', 'B4DJR3'],\n",
    "    \"NOL10\": ['A0A8Q3SHT6'],\n",
    "    \"RPA1\": ['B7Z284'],\n",
    "    \"LIN28B\": ['A0A1B0GTK2'],\n",
    "    \"NOL8\": ['B4DM91', 'F5GWN9'],\n",
    "    \"RPS3A\": ['H0Y9Y4'],\n",
    "    \"EXOSC1\": ['B1AMU7', 'R4GNH9'],\n",
    "    \"UTP18\": ['F6VJE8', 'J3QR85', 'J3KSR7'],\n",
    "    \"NAT10\": ['Q05CW7'],\n",
    "    \"NOP10\": ['A0A8V8TPK8', 'A0A8V8TQT0', 'A0A8V8TP28', 'A0A8V8TPD4'],\n",
    "    \"EBNA1BP2\": ['B4DHA6', 'B4DRX8'],\n",
    "    \"DDX54\": ['F8VRX4'],\n",
    "    \"ZNF330\": ['D6RBR7', 'D6R8Y9', 'D6R9C8'],\n",
    "    \"NOC3L\": ['B4DXL4'],\n",
    "    \"UTP6\": ['B7Z9G4'],\n",
    "    \"SDAD1\": ['F8W8T7', 'D6RC74'],\n",
    "    \"UBTF\": ['E9PKP7', 'B4DNQ1'],\n",
    "    \"MAK16\": ['A0PJ56', 'H0YBV6'],\n",
    "    \"NIFK\": ['C9J6C5']\n",
    "}\n",
    "\n",
    "def append_uniprot_acc(row):\n",
    "    gene_name = row[\"Name\"]\n",
    "    uniprot_id_entry = row[\"UniProt_IDs\"]\n",
    "\n",
    "    # Retrieve the UniProt accessions to be appended\n",
    "    uniprot_accs_to_be_appended = (\n",
    "        missing_gene_name_to_human_acc_dict.get(gene_name)\n",
    "    )\n",
    "    if not uniprot_accs_to_be_appended:\n",
    "        # Leave the `UniProt_IDs` entry unaltered\n",
    "        return uniprot_id_entry\n",
    "\n",
    "    # The current entry is NaN (which is a float)\n",
    "    if pd.isna(uniprot_id_entry):\n",
    "        return \";\".join(uniprot_accs_to_be_appended)\n",
    "    # The current entry is a string comprising at least one UniProt\n",
    "    # accession\n",
    "    else:\n",
    "        return uniprot_id_entry + \";\" + \";\".join(uniprot_accs_to_be_appended)\n",
    "\n",
    "dp_g1_g2_subset_df[\"UniProt_IDs\"] = dp_g1_g2_subset_df.apply(\n",
    "    append_uniprot_acc, axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ea5a0497",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the updated Pandas DataFrame to a TSV file\n",
    "dp_g1_g2_subset_df.to_csv(\n",
    "    \"Dharmacon_pooled_G1_G2_screening_plates_subset_with_missing_\"\n",
    "    \"UniProt_IDs.tsv\",\n",
    "    sep=\"\\t\",\n",
    "    index=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "268fc0a8",
   "metadata": {},
   "source": [
    "#### Z-Scoring of Intensities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3582d5b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The last step involves Z-scoring the intensity..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97fbe289",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bioinformatics",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
