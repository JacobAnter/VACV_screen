{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The purpose of this script is to prepare the input required by the\n",
    "SENSE-PPI LLM for PPI prediction. According to the package's\n",
    "documentation, a TSV file can be passed as input the rows of which\n",
    "contains pairs of proteins to test.\n",
    "\n",
    "Thus, each human protein targeted in the siRNA screen is paired with all\n",
    "440 VACV proteins (comprised in the file \"uniprotkb_taxonomy_id_10254_\n",
    "all_VACV_WR_prots_05_11_2024.fasta\").\n",
    "\n",
    "For the time being, the pairing is performed only for the Qiagen single\n",
    "siRNA subset as it is the most comprehensive, encompassing 20,213 genes.\n",
    "\"\"\"\n",
    "import math\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from biotite.sequence.io import fasta\n",
    "from biotite.database import uniprot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Qiagen subset of the VACV data set and extract the UniProt\n",
    "# IDs\n",
    "dtype_dict = {\n",
    "    \"Ensembl_ID_OnTarget_Ensembl_GRCh38_release_87\": str,\n",
    "    \"Ensembl_ID_OnTarget_NCBI_HeLa_phs000643_v3_p1_c1_HMB\": str,\n",
    "    \"Gene_Description\": str,\n",
    "    \"ID\": str,\n",
    "    \"ID_OnTarget_Ensembl_GRCh38_release_87\": str,\n",
    "    \"ID_OnTarget_Merge\": str,\n",
    "    \"ID_OnTarget_NCBI_HeLa_phs000643_v3_p1_c1_HMB\": str,\n",
    "    \"ID_OnTarget_RefSeq_20170215\": str,\n",
    "    \"ID_manufacturer\": str,\n",
    "    \"Name_alternatives\": str,\n",
    "    \"PLATE_QUALITY_DESCRIPTION\": str,\n",
    "    \"RefSeq_ID_OnTarget_RefSeq_20170215\": str,\n",
    "    \"Seed_sequence_common\": str,\n",
    "    \"WELL_QUALITY_DESCRIPTION\": str,\n",
    "    \"siRNA_error\": str,\n",
    "    \"siRNA_number\": str,\n",
    "    \"Precursor_Name\": str\n",
    "}\n",
    "\n",
    "VACV_df = pd.read_csv(\n",
    "    (\n",
    "        \"VACV_Report_only_valid_single_pooled_siRNA_and_esiRNA_single_\"\n",
    "        \"entries_only_without_Qiagen_mismatches.tsv\"\n",
    "    ),\n",
    "    sep=\"\\t\",\n",
    "    dtype=dtype_dict\n",
    ")\n",
    "\n",
    "Qiagen_df = VACV_df[\n",
    "    VACV_df[\"Manufacturer\"] == \"Qiagen\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bear in mind that some of the targeted genes are pseudogenes and are\n",
    "# thus not associated with any UniProt entries\n",
    "# They have the entry \"Not available\" in the \"UniProt_IDs\" column and\n",
    "# are filtered out\n",
    "Qiagen_df = Qiagen_df[\n",
    "    Qiagen_df[\"UniProt_IDs\"] != \"Not available\"\n",
    "]\n",
    "\n",
    "# Also keep in mind that some entries are composite entries, i.e.\n",
    "# comprise multiple UniProt IDs representing different isoforms\n",
    "# The different isoforms are separated from one another via semicolons\n",
    "Qiagen_uniprot_ids = np.unique([\n",
    "    uniprot_id for entry in Qiagen_df[\"UniProt_IDs\"]\n",
    "    for uniprot_id in entry.split(\";\")\n",
    "]).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "24 proteins are not contained in the reference proteome of Homo sapiens!",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 16\u001b[0m\n\u001b[1;32m      6\u001b[0m ref_proteome_uniprot_ids \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m      7\u001b[0m     header\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m|\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m header \u001b[38;5;129;01min\u001b[39;00m homo_sapiens_reference_proteome_fasta\u001b[38;5;241m.\u001b[39mkeys()\n\u001b[1;32m      9\u001b[0m ]\n\u001b[1;32m     11\u001b[0m contained_in_ref_proteome \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     12\u001b[0m     Qiagen_id \u001b[38;5;129;01min\u001b[39;00m ref_proteome_uniprot_ids\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m Qiagen_id \u001b[38;5;129;01min\u001b[39;00m Qiagen_uniprot_ids\n\u001b[1;32m     14\u001b[0m ]\n\u001b[0;32m---> 16\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mall\u001b[39m(contained_in_ref_proteome), (\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(contained_in_ref_proteome)\u001b[38;5;250m \u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28msum\u001b[39m(contained_in_ref_proteome)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m proteins are not contained in the reference proteome of Homo \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msapiens!\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     20\u001b[0m )\n",
      "\u001b[0;31mAssertionError\u001b[0m: 24 proteins are not contained in the reference proteome of Homo sapiens!"
     ]
    }
   ],
   "source": [
    "# Investigate whether all UniProt IDs also occur in the Homo sapiens\n",
    "# reference proteome\n",
    "homo_sapiens_reference_proteome_fasta = fasta.FastaFile.read(\n",
    "    \"uniprotkb_Homo_sapiens_reference_proteome_06_11_2024.fasta\"\n",
    ")\n",
    "ref_proteome_uniprot_ids = [\n",
    "    header.split(\"|\")[1]\n",
    "    for header in homo_sapiens_reference_proteome_fasta.keys()\n",
    "]\n",
    "\n",
    "contained_in_ref_proteome = [\n",
    "    Qiagen_id in ref_proteome_uniprot_ids\n",
    "    for Qiagen_id in Qiagen_uniprot_ids\n",
    "]\n",
    "\n",
    "assert all(contained_in_ref_proteome), (\n",
    "    f\"{len(contained_in_ref_proteome) - sum(contained_in_ref_proteome)}\"\n",
    "    \" proteins are not contained in the reference proteome of Homo \"\n",
    "    \"sapiens!\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['A0A090N7S4' 'A4D112' 'A4D1Y7' 'B7ZGW9' 'B9EIR0' 'B9EIR1' 'D3DUG6'\n",
      " 'L8E6Z1' 'Q0VGM3' 'Q3MIM1' 'Q4G0H6' 'Q5G014' 'Q6P462' 'Q6PJD4' 'Q6PL46'\n",
      " 'Q6ZW74' 'Q86X61' 'Q8N5R8' 'Q8N5S0' 'Q8N7Z3' 'Q93065' 'Q96QB4' 'Q9BUY1'\n",
      " 'Q9BXE6']\n"
     ]
    }
   ],
   "source": [
    "# It emerges that 24 of the proteins belonging to the Qiagen subset are\n",
    "# not contained in the reference proteome; therefore, their sequences\n",
    "# have to be retrieved by performing a database query\n",
    "ids_not_contained = np.array(Qiagen_uniprot_ids)[\n",
    "    ~np.array(contained_in_ref_proteome)\n",
    "]\n",
    "print(ids_not_contained)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve the sequences of the 24 proteins not contained in the\n",
    "# reference proteome from UniProt\n",
    "uniprot_entries = uniprot.fetch(\n",
    "    ids_not_contained,\n",
    "    format=\"fasta\"\n",
    ")\n",
    "\n",
    "assert len(uniprot_entries) == 24, (\n",
    "    \"An entry has not been retrieved for each of the 24 proteins!\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an empty FASTA file and add the proteins contained in the\n",
    "# reference proteome to it\n",
    "human_prots_Qiagen_subset_fasta = fasta.FastaFile()\n",
    "\n",
    "# The dictionary keys, i.e. FASTA entry headers have to be simplified to\n",
    "# just the UniProt IDs so as to enable effortless dictionary indexing\n",
    "homo_sapiens_reference_proteome_dict = {\n",
    "    uniprot_id: seq_str\n",
    "    for uniprot_id, (_, seq_str) in zip(\n",
    "        ref_proteome_uniprot_ids,\n",
    "        homo_sapiens_reference_proteome_fasta.items()\n",
    "    )\n",
    "}\n",
    "\n",
    "for Qiagen_uniprot_id in Qiagen_uniprot_ids:\n",
    "    try:\n",
    "        sequence = homo_sapiens_reference_proteome_dict[\n",
    "            Qiagen_uniprot_id\n",
    "        ]\n",
    "    except KeyError:\n",
    "        continue\n",
    "\n",
    "    human_prots_Qiagen_subset_fasta[Qiagen_uniprot_id] = sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Also add the 24 proteins not contained in the reference proteome to\n",
    "# the FASTA file\n",
    "for uniprot_entry in uniprot_entries:\n",
    "    # As no target path has been specified, the objects returned by\n",
    "    # `uniprot.fetch()` are StringIO objects, which can be read into a\n",
    "    # FASTA file\n",
    "    current_fasta_file = fasta.FastaFile.read(uniprot_entry)\n",
    "    # Each file contains only one entry; hence, the first and only entry\n",
    "    # is retrieved from the iterator returned by the `.items()` method\n",
    "    header, seq_str = list(current_fasta_file.items())[0]\n",
    "    # Bear in mind that the header has to be simplified to just the\n",
    "    # UniProt ID\n",
    "    header = header.split(\"|\")[1]\n",
    "\n",
    "    # Finally, append the entry to the FASTA file\n",
    "    human_prots_Qiagen_subset_fasta[header] = seq_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert (\n",
    "    len(human_prots_Qiagen_subset_fasta) == len(Qiagen_uniprot_ids)\n",
    "), (\n",
    "    \"An entry hasn't been added to the FASTA file for each protein in \"\n",
    "    \"the Qiagen subset!\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now that the sanity check was successful, the FASTA file is saved to\n",
    "# disk\n",
    "human_prots_Qiagen_subset_fasta.write(\n",
    "    \"human_proteins_in_VACV_data_set_Qiagen_subset.fasta\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/bioinformatics/lib/python3.10/site-packages/biotite/sequence/io/fasta/convert.py:253: UserWarning: ProteinSequence objects do not support selenocysteine (U), occurrences were substituted by cysteine (C)\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Contrary to what I expected, the situation with SENSE-PPI is as\n",
    "# follows: SENSE-PPI requires at least one FASTA file as input the\n",
    "# headers of which consist solely of UniProt IDs and the sequences of\n",
    "# which represent the corresponding protein sequences\n",
    "# Note that this FASTA file has to contain all interacting protein\n",
    "# sequences, i.e. in the case of human-virus PPIs, this file has to\n",
    "# contain both human and virus sequences\n",
    "# In case of just providing the aforementioned FASTA file, all possible\n",
    "# interaction pairs are computed\n",
    "# However, if interaction scores are supposed to be computed only for\n",
    "# selected protein pairs, then a second file must be passed as input\n",
    "# This second file is  a TSV file the rows of which represent protein\n",
    "# pairs to compute the interaction probability for; the proteins are\n",
    "# represented by their UniProt ID; the TSV must have at least two\n",
    "# columns harbouring the UniProt IDs of interaction pairs, it may\n",
    "# optionally contain a third column harbouring labels (i.e. 1 for \"true\"\n",
    "# and 0 for \"false\")\n",
    "\n",
    "# Therefore, the two FASTA files containing the human and VACV sequences\n",
    "# have to be merged\n",
    "# This can be easily accomplished using the `fasta.set_sequences()`\n",
    "# function\n",
    "human_prots_Qiagen_subset_and_all_VACV_WR_prots_fasta = fasta.FastaFile()\n",
    "\n",
    "# Obtain a dictionary for each of the two files\n",
    "VACV_WR_prots_fasta = fasta.FastaFile.read(\n",
    "    \"uniprotkb_taxonomy_id_10254_all_VACV_WR_prots_05_11_2024.fasta\"\n",
    ")\n",
    "VACV_WR_fasta_dict = fasta.get_sequences(VACV_WR_prots_fasta)\n",
    "# Bear in mind that the VACV headers are unaltered, so the UniProt IDs\n",
    "# still have to be extracted\n",
    "VACV_WR_fasta_dict = {\n",
    "    header.split(\"|\")[1]: seq_object\n",
    "    for header, seq_object in VACV_WR_fasta_dict.items()\n",
    "}\n",
    "\n",
    "human_prots_Qiagen_subset_fasta = fasta.FastaFile.read(\n",
    "    \"human_proteins_in_VACV_data_set_Qiagen_subset.fasta\"\n",
    ")\n",
    "human_prots_Qiagen_subset_fasta_dict = fasta.get_sequences(\n",
    "    human_prots_Qiagen_subset_fasta\n",
    ")\n",
    "\n",
    "# Now, merge the two dictionaries and write the sequences contained in\n",
    "# the resulting dictionary into a new FASTA file\n",
    "# Merging can be easily achieved via the double asterisk (**) operator\n",
    "# used to unpack dictionaries\n",
    "human_Qiagen_and_VACV_WR_prots_dict = {\n",
    "    **human_prots_Qiagen_subset_fasta_dict, **VACV_WR_fasta_dict\n",
    "}\n",
    "\n",
    "human_Qiagen_and_VACV_WR_prots_fasta = fasta.FastaFile()\n",
    "fasta.set_sequences(\n",
    "    human_Qiagen_and_VACV_WR_prots_fasta,\n",
    "    human_Qiagen_and_VACV_WR_prots_dict\n",
    ")\n",
    "\n",
    "n_human_Qiagen_prots = len(human_prots_Qiagen_subset_fasta)\n",
    "n_VACV_WR_prots = len(VACV_WR_prots_fasta)\n",
    "\n",
    "assert (\n",
    "    len(human_Qiagen_and_VACV_WR_prots_fasta)\n",
    "    ==\n",
    "    n_human_Qiagen_prots + n_VACV_WR_prots\n",
    "), (\n",
    "    \"Something went wrong while merging the two dictionaries!\"\n",
    ")\n",
    "\n",
    "human_Qiagen_and_VACV_WR_prots_fasta.write(\n",
    "    \"human_prots_Qiagen_subset_and_all_VACV_WR_prots.fasta\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 442 proteins in the VACV WR proteome, whereas the Qiagen subset\n",
      "of the VACV screen encompasses 18,610 proteins.\n"
     ]
    }
   ],
   "source": [
    "# Now, the TSV file itemising protein pairs to test is turned to\n",
    "# A DataFrame can be created in various ways, one of which consists of\n",
    "# using a dictionary with column names as keys and lists representing\n",
    "# the column contents as values\n",
    "# All human proteins in the Qiagen subset are supposed to be paired with\n",
    "# each and every VACV strain Western Reserve protein\n",
    "# Hence, for the column harbouring human proteins, a list is created\n",
    "# wherein each human protein is repeated as often as there are VACV WR\n",
    "# proteins, i.e. 442 times (the protein sequences have been downladed\n",
    "# including isoforms, which is why the FASTA file encompasses 442\n",
    "# instead of 440 entries)\n",
    "# Conversely, for the column harbouring VACV proteins, the entire VACV\n",
    "# WR proteome is repeated as often as there are human proteins in the\n",
    "# Qiagen subset\n",
    "print(\n",
    "    f\"There are {n_VACV_WR_prots} proteins in the VACV WR proteome, \"\n",
    "    \"whereas the Qiagen subset\\nof the VACV screen encompasses \"\n",
    "    f\"{n_human_Qiagen_prots:,} proteins.\"\n",
    ")\n",
    "\n",
    "human_column_list = [\n",
    "    uniprot_id\n",
    "    for uniprot_id in Qiagen_uniprot_ids\n",
    "    for _ in range(n_VACV_WR_prots)\n",
    "]\n",
    "\n",
    "VACV_WR_uniprot_IDs = [\n",
    "    header.split(\"|\")[1] for header in VACV_WR_prots_fasta.keys()\n",
    "]\n",
    "VACV_column_list = VACV_WR_uniprot_IDs * n_human_Qiagen_prots\n",
    "\n",
    "assert len(human_column_list) == len(VACV_column_list), (\n",
    "    \"Something went wrong while creating the column lists!\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame using the two lists and save the DataFrame to a TSV\n",
    "# file\n",
    "prot_pair_dict = {\n",
    "    \"Human_protein\": human_column_list,\n",
    "    \"VACV_protein\": VACV_column_list\n",
    "}\n",
    "\n",
    "prot_pair_df = pd.DataFrame(data=prot_pair_dict)\n",
    "\n",
    "prot_pair_df.to_csv(\n",
    "    \"PPI_pairs_between_Qiagen_subset_and_VACV_WR_proteome.tsv\",\n",
    "    sep=\"\\t\",\n",
    "    header=False,\n",
    "    index=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Also create a TSV file encompassing only five PPI pairs for testing\n",
    "# purposes\n",
    "test_df = prot_pair_df.iloc[:5]\n",
    "test_df.to_csv(\n",
    "    \"SENSE-PPI_test_prot_pairs.tsv\",\n",
    "    sep=\"\\t\",\n",
    "    header=False,\n",
    "    index=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a FASTA file containing the corresponding protein sequences\n",
    "uniprot_ids_in_test_tsv = (\n",
    "    test_df[\"Human_protein\"].unique().tolist()\n",
    "    +\n",
    "    test_df[\"VACV_protein\"].unique().tolist()\n",
    ")\n",
    "\n",
    "test_fasta = fasta.FastaFile()\n",
    "\n",
    "for test_uniprot_id in uniprot_ids_in_test_tsv:\n",
    "    seq_str = human_Qiagen_and_VACV_WR_prots_fasta[test_uniprot_id]\n",
    "    test_fasta[test_uniprot_id] = seq_str\n",
    "\n",
    "test_fasta.write(\n",
    "    \"SENSE-PPI_test_prot_seqs.fasta\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Amount of interaction pairs between the Qiagen subset and the VACV strain\n",
      "Western Reserve proteome: 8,225,620\n",
      "Amount of protein sequences from both the Qiagen subset and the VACV\n",
      "strain WR proteome: 19,052\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    \"Amount of interaction pairs between the Qiagen subset and the \"\n",
    "    f\"VACV strain\\nWestern Reserve proteome: {len(prot_pair_df):,}\"\n",
    ")\n",
    "\n",
    "print(\n",
    "    \"Amount of protein sequences from both the Qiagen subset and the \"\n",
    "    \"VACV\\nstrain WR proteome: \"\n",
    "    f\"{len(human_Qiagen_and_VACV_WR_prots_fasta):,}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19052\n"
     ]
    }
   ],
   "source": [
    "# The TSV file encompasses more than 8 million interactions, which are\n",
    "# way too many to be processed in one batch job\n",
    "# For this reason, the TSV file is split into 17 chunks each\n",
    "# encompassing 500,000 interaction pairs (except the last one)\n",
    "# The same is done to the FASTA file harbouring the human and VACV\n",
    "# protein sequences\n",
    "prot_pair_df = pd.read_csv(\n",
    "    \"SENSE-PPI/PPI_pairs_between_Qiagen_subset_and_VACV_WR_proteome.tsv\",\n",
    "    sep=\"\\t\"\n",
    ")\n",
    "\n",
    "# Providing the FASTA file harbouring all sequences causes an Out of\n",
    "# Memory (OOM) error\n",
    "# Therefore, a separate FASTA file is generated for each chunk\n",
    "human_Qiagen_and_VACV_WR_prots_fasta = fasta.FastaFile.read(\n",
    "    \"SENSE-PPI/human_prots_Qiagen_subset_and_all_VACV_WR_prots.fasta\"\n",
    ")\n",
    "\n",
    "CHUNK_SIZE = 500000\n",
    "n_chunks = math.ceil(len(prot_pair_df) / CHUNK_SIZE)\n",
    "\n",
    "prot_pair_chunks = [\n",
    "    prot_pair_df.iloc[i * CHUNK_SIZE : (i + 1) * CHUNK_SIZE]\n",
    "    for i in range(n_chunks)\n",
    "]\n",
    "\n",
    "# Save each of the chunks to a separate TSV file\n",
    "for i, prot_pair_chunk in enumerate(prot_pair_chunks):\n",
    "    # Save a chunk from the TSV file to a new TSV file\n",
    "    prot_pair_chunk.to_csv(\n",
    "        (\n",
    "            \"SENSE-PPI/PPI_pairs_between_Qiagen_subset_and_VACV_WR_\"\n",
    "            f\"proteome_chunk_{i}_size_{CHUNK_SIZE:,}.tsv\"\n",
    "        ),\n",
    "        sep=\"\\t\",\n",
    "        header=False,\n",
    "        index=False\n",
    "    )\n",
    "\n",
    "    # Extract the corresponding chunk from the FASTA file\n",
    "    # This requires determining the unique UniProt IDs in the current\n",
    "    # chunk\n",
    "    # As `pandas.unique()` accepts exclusively one-dimensional\n",
    "    # array-like objects as input, it is resorted to employing the\n",
    "    # function in conjunction with `pandas.melt()`, which transforms a\n",
    "    # DataFrame from a wide format into a long format\n",
    "    # This yields a DataFrame encompassing only two columns bearing the\n",
    "    # name \"variable\" and \"value\", respectively\n",
    "    # Finally, the `unique()` method is applied to the `value` column of\n",
    "    # the DataFrame\n",
    "    unique_prot_ids_current_chunk = pd.melt(\n",
    "        prot_pair_chunk\n",
    "    )[\"value\"].unique()\n",
    "\n",
    "    current_chunk_fasta = fasta.FastaFile()\n",
    "    for uniprot_id in unique_prot_ids_current_chunk:\n",
    "        current_chunk_fasta[uniprot_id] = (\n",
    "            human_Qiagen_and_VACV_WR_prots_fasta[uniprot_id]\n",
    "        )\n",
    "    current_chunk_fasta.write(\n",
    "        \"SENSE-PPI/\"\n",
    "        f\"human_prots_Qiagen_subset_and_VACV_WR_prots_seqs_chunk_{i}_\"\n",
    "        f\"size_{CHUNK_SIZE:,}.fasta\"\n",
    "    )\n",
    "\n",
    "    # Additionally, a TXT file is created in which each line is\n",
    "    # populated with one UniProt ID of the current chunk\n",
    "    # This is done with the intention to automate the copying of\n",
    "    # embedding files into the corresponding directories\n",
    "    # Bear in mind that in the context of working with files, the `with`\n",
    "    # context manager is preferred as it automatically takes care of\n",
    "    # closing files, even in case of exceptions/errors\n",
    "    with open(f\"SENSE-PPI/UniProt_IDs_chunk_{i}.txt\", \"w\") as f:\n",
    "        prot_ids_with_newlines = [\n",
    "            uniprot_id if i == 0 else \"\\n\" + uniprot_id\n",
    "            for i, uniprot_id in enumerate(unique_prot_ids_current_chunk)\n",
    "        ]\n",
    "        f.writelines(prot_ids_with_newlines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weirdly enough, while all other chunks could be processed without any\n",
    "# problems, OOM errors for chunks 11 as well as 13\n",
    "# Therefore, the aforementioned chunks are further split into three\n",
    "# sub-chunks each\n",
    "SUB_CHUNK_SIZE = math.ceil(500000 / 3)\n",
    "\n",
    "# Read in the two chunks\n",
    "prot_pairs_chunk_11 = pd.read_csv(\n",
    "    (\n",
    "        \"SENSE-PPI/Files_per_chunk_except_chunk_0/PPI_pairs_between_\"\n",
    "        \"Qiagen_subset_and_VACV_WR_proteome_chunk_11_size_500,000.tsv\"\n",
    "    ),\n",
    "    sep=\"\\t\"\n",
    ")\n",
    "prot_pairs_chunk_13 = pd.read_csv(\n",
    "    (\n",
    "        \"SENSE-PPI/Files_per_chunk_except_chunk_0/PPI_pairs_between_\"\n",
    "        \"Qiagen_subset_and_VACV_WR_proteome_chunk_13_size_500,000.tsv\"\n",
    "    ),\n",
    "    sep=\"\\t\"\n",
    ")\n",
    "\n",
    "# Also read in the FASTA file encompassing all human as well as VACV\n",
    "# proteins\n",
    "human_Qiagen_and_VACV_WR_prots_fasta = fasta.FastaFile.read(\n",
    "    \"SENSE-PPI/human_prots_Qiagen_subset_and_all_VACV_WR_prots.fasta\"\n",
    ")\n",
    "\n",
    "prot_pair_subchunks_11 = [\n",
    "    prot_pairs_chunk_11.iloc[i * SUB_CHUNK_SIZE : (i + 1) * SUB_CHUNK_SIZE]\n",
    "    for i in range(3)\n",
    "]\n",
    "prot_pair_subchunks_13 = [\n",
    "    prot_pairs_chunk_13.iloc[i * SUB_CHUNK_SIZE : (i + 1) * SUB_CHUNK_SIZE]\n",
    "    for i in range(3)\n",
    "]\n",
    "\n",
    "# Save each of the chunks to a separate TSV file\n",
    "# First, deal with the sub-chunks of chunk 11\n",
    "for i, prot_pair_sub_chunk in enumerate(prot_pair_subchunks_11):\n",
    "    prot_pair_sub_chunk.to_csv(\n",
    "        (\n",
    "            \"SENSE-PPI/PPI_pairs_between_Qiagen_subset_and_VACV_WR_\"\n",
    "            f\"proteome_chunk_11_{i}_size_{SUB_CHUNK_SIZE:,}.tsv\"\n",
    "        ),\n",
    "        sep=\"\\t\",\n",
    "        header=False,\n",
    "        index=False\n",
    "    )\n",
    "\n",
    "    # Extract the corresponding chunk from the FASTA file\n",
    "    unique_prot_ids_current_sub_chunk = pd.melt(\n",
    "        prot_pair_sub_chunk\n",
    "    )[\"value\"].unique()\n",
    "\n",
    "    current_sub_chunk_fasta = fasta.FastaFile()\n",
    "    for uniprot_id in unique_prot_ids_current_sub_chunk:\n",
    "        current_sub_chunk_fasta[uniprot_id] = (\n",
    "            human_Qiagen_and_VACV_WR_prots_fasta[uniprot_id]\n",
    "        )\n",
    "    current_sub_chunk_fasta.write(\n",
    "        \"SENSE-PPI/\"\n",
    "        \"human_prots_Qiagen_subset_and_VACV_WR_prots_seqs_chunk_11_\"\n",
    "        f\"{i}_size_{SUB_CHUNK_SIZE:,}.fasta\"\n",
    "    )\n",
    "\n",
    "# Now, turn to chunk 13\n",
    "for i, prot_pair_sub_chunk in enumerate(prot_pair_subchunks_13):\n",
    "    prot_pair_sub_chunk.to_csv(\n",
    "        (\n",
    "            \"SENSE-PPI/PPI_pairs_between_Qiagen_subset_and_VACV_WR_\"\n",
    "            f\"proteome_chunk_13_{i}_size_{SUB_CHUNK_SIZE:,}.tsv\"\n",
    "        ),\n",
    "        sep=\"\\t\",\n",
    "        header=False,\n",
    "        index=False\n",
    "    )\n",
    "\n",
    "    # Extract the corresponding chunk from the FASTA file\n",
    "    unique_prot_ids_current_sub_chunk = pd.melt(\n",
    "        prot_pair_sub_chunk\n",
    "    )[\"value\"].unique()\n",
    "\n",
    "    current_sub_chunk_fasta = fasta.FastaFile()\n",
    "    for uniprot_id in unique_prot_ids_current_sub_chunk:\n",
    "        current_sub_chunk_fasta[uniprot_id] = (\n",
    "            human_Qiagen_and_VACV_WR_prots_fasta[uniprot_id]\n",
    "        )\n",
    "    current_sub_chunk_fasta.write(\n",
    "        \"SENSE-PPI/\"\n",
    "        \"human_prots_Qiagen_subset_and_VACV_WR_prots_seqs_chunk_13_\"\n",
    "        f\"{i}_size_{SUB_CHUNK_SIZE:,}.fasta\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Contrary to expectation, splitting each of the two chunks into three\n",
    "# sub-chunks did not resolve the issue\n",
    "# As a last resort, both chunks are processed with CPU only"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bioinformatics",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
