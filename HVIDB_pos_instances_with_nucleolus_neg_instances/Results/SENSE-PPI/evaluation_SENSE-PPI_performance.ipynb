{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nThe purpose of this Jupyter notebook is to evaluate the performance of\\nSENSE-PPI, which serves as one of the three published benchmark models.\\n\\nBeyond just computing five different metrics (accuracy, precision,\\nrecall, F1-score, specificity) as average across the 10 splits, the ROC\\nAUC score is also computed, which summarises a classifier's performance\\nacross different classification thresholds.\\n\""
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "The purpose of this Jupyter notebook is to evaluate the performance of\n",
    "SENSE-PPI, which serves as one of the three published benchmark models.\n",
    "\n",
    "Beyond just computing five different metrics (accuracy, precision,\n",
    "recall, F1-score, specificity) as average across the 10 splits, the ROC\n",
    "AUC score is also computed, which summarises a classifier's performance\n",
    "across different classification thresholds.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As 10-fold cross-validation is supposed to be performed, there are 10\n",
    "# different data set splits and thus 10 different test sets\n",
    "# However, bear in mind that due to memory constraints, each test set\n",
    "# has been subdivided into four chunks\n",
    "# Therefore, the first step consists of stitching the chunks belonging\n",
    "# to one test set together\n",
    "# Also bear in mind that this chunk concatenation has to be performed\n",
    "# twice per test set as SENSE-PPI outputs two output files\n",
    "\n",
    "# Iterate over the 10 test sets\n",
    "for i in range(10):\n",
    "    # Iterate over the 4 different chunks each test set has been\n",
    "    # subdivided into and append them to a list\n",
    "    all_preds_list = []\n",
    "    pos_preds_list = []\n",
    "\n",
    "    for j in range(4):\n",
    "        all_preds_list.append(\n",
    "            pd.read_csv(\n",
    "                f\"results_split_{i}/predictions_on_VACV_WR_pos_and_\"\\\n",
    "                f\"neg_data_set_test_set_split_{i}_chunk_{j}_without_\"\\\n",
    "                \"training.tsv\",\n",
    "                sep=\"\\t\"\n",
    "            )\n",
    "        )\n",
    "        pos_preds_list.append(\n",
    "            pd.read_csv(\n",
    "                f\"results_split_{i}/predictions_on_VACV_WR_pos_and_\"\\\n",
    "                f\"neg_data_set_test_set_split_{i}_chunk_{j}_without_\"\\\n",
    "                \"training_positive_interactions.tsv\",\n",
    "                sep=\"\\t\"\n",
    "            )\n",
    "        )\n",
    "\n",
    "    all_predictions_full = pd.concat(all_preds_list, ignore_index=True)\n",
    "    pos_predictions_full = pd.concat(pos_preds_list, ignore_index=True)\n",
    "\n",
    "    all_predictions_full.to_csv(\n",
    "        f\"results_split_{i}/predictions_on_VACV_WR_pos_and_neg_data_\"\\\n",
    "        f\"set_test_set_split_{i}_without_training.tsv\",\n",
    "        sep=\"\\t\",\n",
    "        index=False\n",
    "    )\n",
    "    pos_predictions_full.to_csv(\n",
    "        f\"results_split_{i}/predictions_on_VACV_WR_pos_and_neg_data_\"\\\n",
    "        f\"set_test_set_split_{i}_without_training_positive_interactions.tsv\",\n",
    "        sep=\"\\t\",\n",
    "        index=False\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the groud truth, i.e. the combined VACV WR data encompassing\n",
    "# confirmed positive PPIs as well as reliable negative PPIs involving\n",
    "# nucleolus proteins\n",
    "VACV_WR_PPIs_ground_truth_df = pd.read_csv(\n",
    "    \"/Users/jacobanter/Documents/Code/VACV_screen/HVIDB_pos_instances\"\\\n",
    "    \"_with_nucleolus_neg_instances/VACV_WR_pos_and_nucleolus_prots_\"\\\n",
    "    \"neg_PPI_instances.tsv\",\n",
    "    sep=\"\\t\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In order to be able to utilise scikit-learn's `confusion_matrix`\n",
    "# class, the ground truth labels have to be extracted for each and every\n",
    "# test set\n",
    "ground_truth_label_list = []\n",
    "\n",
    "for i in range(10):\n",
    "    # Load the TSV with predictions for the current test set\n",
    "    # This is done instead of loading e.g. the original test set TSV\n",
    "    # file as the PPI pairs must have the same ordering in order for the\n",
    "    # metrics to be computed correctly\n",
    "    current_test_set_preds_df = pd.read_csv(\n",
    "        f\"results_split_{i}/predictions_on_VACV_WR_pos_and_neg_data_\"\\\n",
    "        f\"set_test_set_split_{i}_without_training.tsv\",\n",
    "        sep=\"\\t\"\n",
    "    )\n",
    "\n",
    "    current_test_set_ground_truth_labels = []\n",
    "\n",
    "    for _, row in current_test_set_preds_df.iterrows():\n",
    "        human_uniprot_id = row[\"seq1\"]\n",
    "        VACV_uniprot_id = row[\"seq2\"]\n",
    "\n",
    "        ground_truth_label = VACV_WR_PPIs_ground_truth_df.loc[\n",
    "            (VACV_WR_PPIs_ground_truth_df[\"Human_prot\"] == human_uniprot_id)\n",
    "            &\n",
    "            (VACV_WR_PPIs_ground_truth_df[\"VACV_prot\"] == VACV_uniprot_id)\n",
    "        ][\"label\"].iloc[0]\n",
    "        \n",
    "        current_test_set_ground_truth_labels.append(ground_truth_label)\n",
    "    \n",
    "    ground_truth_label_list.append(current_test_set_ground_truth_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, five metrics are computed for each and every split test set\n",
    "# In detail, these five metrics are accuracy, precision, recall,\n",
    "# F1-score and specificity\n",
    "# To this end, scikit-learn's `confusion_matrix` class is utilised\n",
    "\n",
    "# Store the metrics for each test set in corresponding lists\n",
    "accuracy_list = []\n",
    "precision_list = []\n",
    "recall_list = []\n",
    "f1_score_list = []\n",
    "specificity_list = []\n",
    "\n",
    "# Iterate over the 10 test sets\n",
    "for i, current_ground_truths in enumerate(ground_truth_label_list):\n",
    "    current_split_preds_df = pd.read_csv(\n",
    "        f\"results_split_{i}/predictions_on_VACV_WR_pos_and_neg_data_\"\\\n",
    "        f\"set_test_set_split_{i}_without_training.tsv\",\n",
    "        sep=\"\\t\"\n",
    "    )\n",
    "    current_predictions = current_split_preds_df[\"label\"].to_list()\n",
    "\n",
    "    # Extract the predicted labels and feed them along with the ground\n",
    "    # truth labels into the `confusion_matrix` class\n",
    "    cm = confusion_matrix(\n",
    "        current_ground_truths,\n",
    "        current_predictions\n",
    "    )\n",
    "    \n",
    "    # Accuracy is defined as the proportion of correct predictions in\n",
    "    # all predictions made by the model and is hence computed as\n",
    "    # follows:\n",
    "    # (# correct predictions) / (# all predictions)\n",
    "    # = (TP + TN) /(TP + TN + FP + FN)\n",
    "    accuracy = (cm[1,1] + cm[0,0]) / (cm[1,1] + cm[0,0] + cm[0,1] + cm[1,0])\n",
    "    # Precision is defined as the proportion of correct positive\n",
    "    # predictions in all positive predictions and is thus computed as\n",
    "    # follows:\n",
    "    # (# true positives) / (# positive predictions)\n",
    "    # = TP / (TP + FP)\n",
    "    precision = cm[1,1] / (cm[1,1] + cm[0,1])\n",
    "    # Recall, also known as sensitivity, is defined as the proportion of\n",
    "    # correctly identified positive instances in all positive instances\n",
    "    # and is thus computed as follows:\n",
    "    # (# true positives) / (# positive instances in data set)\n",
    "    # = TP / (TP + FN)\n",
    "    recall = cm[1,1] / (cm[1,1] + cm[1,0])\n",
    "    # F1-score is a metric incorporating both precision and recall; to\n",
    "    # be more precise, the F1-score is the harmonic mean of precision\n",
    "    # and recall\n",
    "    # It is defined as follows:\n",
    "    # 2*TP / (2*TP + FP + FN)\n",
    "    f1_score = 2*cm[1,1] / (2*cm[1,1] + cm[0,1] + cm[1,0])\n",
    "    # Specificity, which can be considered the opposite of recall, is\n",
    "    # defined as the proportion of correctly identified negative\n",
    "    # instances in all negative instances and is hence computed as\n",
    "    # follows:\n",
    "    # (# true negatives) / (# negative instances in data set)\n",
    "    # = TN / (TN + FP)\n",
    "    specificity = cm[0,0] / (cm[0,0] + cm[0,1])\n",
    "\n",
    "    accuracy_list.append(accuracy)\n",
    "    precision_list.append(precision)\n",
    "    recall_list.append(recall)\n",
    "    f1_score_list.append(f1_score)\n",
    "    specificity_list.append(specificity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 10-fold cross-validation, the metrics for SENSE-PPI are as follows:\n",
      "Accuracy:    1.0 ± 0.0\n",
      "Precision:   1.0 ± 0.0\n",
      "Recall:      1.0 ± 0.0\n",
      "F1-Score:    1.0 ± 0.0\n",
      "Specificity: 1.0 ± 0.0\n"
     ]
    }
   ],
   "source": [
    "# Compute the mean and the standard deviation for each of the five\n",
    "# metrics\n",
    "accuracy_mean = np.mean(accuracy_list)\n",
    "accuracy_std = np.std(accuracy_list)\n",
    "\n",
    "precision_mean = np.mean(precision_list)\n",
    "precision_std = np.std(precision_list)\n",
    "\n",
    "recall_mean = np.mean(recall_list)\n",
    "recall_std = np.std(recall_list)\n",
    "\n",
    "f1_score_mean = np.mean(f1_score_list)\n",
    "f1_score_std = np.std(f1_score_list)\n",
    "\n",
    "specificity_mean = np.mean(specificity_list)\n",
    "specificity_std = np.std(specificity_list)\n",
    "\n",
    "# Regarding string padding by means of string methods such as\n",
    "# `.ljust()`, it must be noted that if string continuation by e.g.\n",
    "# parentheses is used, the entire text preceding a certain point will be\n",
    "# considered contiguous\n",
    "# Thus, in order to apply string padding to a defined string, it is\n",
    "# advisable to separate that string from the surrounding ones by e.g.\n",
    "# commas or the plus operator\n",
    "metrics_result_text = (\n",
    "    \"Using 10-fold cross-validation, the metrics for SENSE-PPI are as \"\n",
    "    \"follows:\\n\" +\n",
    "    \"Accuracy:\".ljust(13) + f\"{accuracy_mean} \\xB1 {accuracy_std}\\n\" +\n",
    "    \"Precision:\".ljust(13) + f\"{precision_mean} \\xB1 {precision_std}\\n\" +\n",
    "    \"Recall:\".ljust(13) + f\"{recall_mean} \\xB1 {recall_std}\\n\" +\n",
    "    \"F1-Score:\".ljust(13) + f\"{f1_score_mean} \\xB1 {f1_score_std}\\n\" +\n",
    "    \"Specificity:\".ljust(13) + f\"{specificity_mean} \\xB1 {specificity_std}\"\n",
    ")\n",
    "\n",
    "print(metrics_result_text)\n",
    "\n",
    "# Bear in mind that in the context of working with files, the `with`\n",
    "# context manager is preferred as it automatically takes care of closing\n",
    "# files, even in case of errors/exceptions\n",
    "with open(\n",
    "    \"SENSE-PPI_results_10-fold_cross-validation_on_combined_VACV_WR_\"\\\n",
    "    \"data_set_without_training.txt\",\n",
    "    \"w\"\n",
    ") as f:\n",
    "    f.write(metrics_result_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, address the computation of the ROC AUC score\n",
    "# A Receiver Operator Characteristic curve, abbreviated as ROC curve,\n",
    "# summarises a classifier's performance across different classification\n",
    "# thresholds\n",
    "# Conceptually, each point of an ROC curve represents a confusion matrix\n",
    "# obtained for the respective classification threshold\n",
    "# Thus, an ROC curve visually summarises the information contained in\n",
    "# multiple confusion matrices by plotting the True Positive Rate (TPR)\n",
    "# against the False Positive Rate (FPR) for each confusion matrix/\n",
    "# threshold\n",
    "# The ROC AUC score, as its name already suggests, is the area under the\n",
    "# curve of the ROC curve\n",
    "# The ROC AUC score allows the comparison of different classifiers with\n",
    "# greater ROC AUC scores indicating superior predictive capabilities\n",
    "# This is due to the fact that points to the left of the diagonal\n",
    "# indicate a threshold for which the TPR exceeds the FPR\n",
    "\n",
    "# Import libraries required for ROC AUC score computation as well as the\n",
    "# visualisation of the ROC curve\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average ROC AUC score across the 10 test sets\n",
      "is 0.718051512823087 ± 0.04692949363532433.\n"
     ]
    }
   ],
   "source": [
    "# As 10-fold cross-validation has been performed, the ROC AUC score is\n",
    "# computed as follows:\n",
    "# It is iterated over the 10 test sets, strictly speaking over their\n",
    "# ground truth values as well as their predicted probabilities\n",
    "# Then, for each of them, the ROC AUC score is computed\n",
    "# Finally, the average ROC AUC score is taken across the 10 test sets\n",
    "\n",
    "roc_auc_score_list = []\n",
    "\n",
    "for i, current_ground_truths in enumerate(ground_truth_label_list):\n",
    "    # Access the predicted probabilities of the test set at hand\n",
    "    current_split_preds_df = pd.read_csv(\n",
    "        f\"results_split_{i}/predictions_on_VACV_WR_pos_and_neg_data_\"\\\n",
    "        f\"set_test_set_split_{i}_without_training.tsv\",\n",
    "        sep=\"\\t\"\n",
    "    )\n",
    "\n",
    "    current_probs = current_split_preds_df[\"preds\"].to_list()\n",
    "\n",
    "    current_roc_auc_score = roc_auc_score(\n",
    "        current_ground_truths, current_probs\n",
    "    )\n",
    "    \n",
    "    roc_auc_score_list.append(current_roc_auc_score)\n",
    "\n",
    "roc_auc_score_mean = np.mean(roc_auc_score_list)\n",
    "roc_auc_score_std = np.std(roc_auc_score_list)\n",
    "\n",
    "print(\n",
    "    \"The average ROC AUC score across the 10 test sets\\nis \"\n",
    "    f\"{roc_auc_score_mean} \\xB1 {roc_auc_score_std}.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bioinformatics",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
